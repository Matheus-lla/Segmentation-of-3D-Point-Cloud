{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ground Plane Fitting and Scan Line Run for 3D LiDAR\n",
    "\n",
    "Ground Plane Fitting (GPF) and Naive Baseline for 3D LiDAR Segmentation\n",
    "\n",
    "This notebook implements ground segmentation using the Ground Plane Fitting (GPF) algorithm \n",
    "proposed in:\n",
    "\n",
    "\"Fast Segmentation of 3D Point Clouds: A Paradigm on LiDAR Data for Autonomous Vehicle Applications\"\n",
    "by D. Zermas, I. Izzat, and N. Papanikolopoulos, 2017.\n",
    "\n",
    "The implementation also includes a naive baseline method for comparison, as well as \n",
    "basic clustering and visualization tools."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pypcd import pypcd  # Use local pypcd version (Python 3 compatible, no lzf support)\n",
    "from sklearn.decomposition import PCA  # Used for plane fitting\n",
    "from sklearn.neighbors import NearestNeighbors  # Used for spatial clustering\n",
    "import pptk  # To install: pip install pptk\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import yaml\n",
    "# Note: Ubuntu users may need to fix libz.so.1 symlink issue (see: https://github.com/heremaps/pptk/issues/3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Number of lowest points to use for initial ground seed estimation (LPR)\n",
    "# N_LPR = 1000\n",
    "\n",
    "# # Number of iterations for ground plane refinement\n",
    "# N_ITERATIONS = 6\n",
    "\n",
    "# # Height threshold above the LPR to select initial seed points\n",
    "# TH_SEEDS = 0.4\n",
    "\n",
    "# # Distance threshold to classify a point as ground\n",
    "# TH_DISTANCE_FROM_PLANE = 0.2\n",
    "\n",
    "# RADIUS_FOR_NEAREST_NEIGHBORS = 0.5\n",
    "\n",
    "# MIN_POINTS_FOR_CLUSTER = 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Baseline Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_ground_extractor(point_cloud, num_lowest_points):\n",
    "    \"\"\"\n",
    "    Naive ground extraction method (baseline).\n",
    "    \n",
    "    This simple method selects the points with the lowest Z values \n",
    "    and assumes they belong to the ground surface. It does not model \n",
    "    the ground plane and is used as a baseline for comparison against \n",
    "    more robust algorithms like Ground Plane Fitting (GPF).\n",
    "    \n",
    "    Args:\n",
    "        point_cloud (np.ndarray): N x 3 or N x D array of point cloud data.\n",
    "        num_lowest_points (int): Number of points with lowest Z values to classify as ground.\n",
    "    \n",
    "    Returns:\n",
    "        ground_indices (np.ndarray): Indices of the selected ground points.\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 1: Sort points by Z axis (height)\n",
    "    sorted_indices = np.argsort(point_cloud[:, 2])\n",
    "\n",
    "    # Step 2: Take the first N indices as ground\n",
    "    ground_indices = sorted_indices[:num_lowest_points]\n",
    "\n",
    "    return ground_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ground Plane Fitting (GPF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpf_extract_initial_seeds_ids(point_cloud, number_of_lowest_points=1000, threshold_seeds=0.4):\n",
    "    \"\"\"\n",
    "    Extract initial seed points for ground plane estimation (GPF).\n",
    "    \n",
    "    Args:\n",
    "        point_cloud (np.ndarray): N x 3 array of points (x, y, z).\n",
    "        number_of_lowest_points (int): number of lowest Z points to average as LPR.\n",
    "        threshold_seeds (float): threshold to select seeds close to LPR height.\n",
    "    \n",
    "    Returns:\n",
    "        seeds_ids (np.ndarray): indices of points selected as initial seeds.\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 1: Sort the point cloud by Z axis (height)\n",
    "    sorted_indices = np.argsort(point_cloud[:, 2])          # Get indices sorted by height\n",
    "    sorted_points = point_cloud[sorted_indices]             # Apply sorting\n",
    "\n",
    "    # Step 2: Compute LPR (Lowest Point Representative)\n",
    "    LPR_height = np.mean(sorted_points[:number_of_lowest_points, 2])\n",
    "\n",
    "    # Step 3: Select points that are within threshold distance from LPR\n",
    "    seed_mask = sorted_points[:, 2] < (LPR_height + threshold_seeds)\n",
    "    seeds_ids = sorted_indices[seed_mask]\n",
    "\n",
    "    return seeds_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def gpf_estimate_plane(seed_points):\n",
    "#     \"\"\"\n",
    "#     Estimate ground plane from seed points using PCA.\n",
    "    \n",
    "#     Args:\n",
    "#         seed_points (np.ndarray): N x 3 or N x D array of selected ground seed points.\n",
    "    \n",
    "#     Returns:\n",
    "#         normal_vector (np.ndarray): Unit normal vector [a, b, c] of the estimated plane.\n",
    "#         d (float): Plane offset in the equation ax + by + cz + d = 0\n",
    "#     \"\"\"\n",
    "\n",
    "#     # Step 1: Compute the mean position of the seed points\n",
    "#     mean_point = np.mean(seed_points, axis=0)\n",
    "\n",
    "#     # Step 2: Center the point cloud around the origin\n",
    "#     centered = seed_points - mean_point\n",
    "\n",
    "#     # Step 3: Use PCA to estimate the plane's normal vector\n",
    "#     pca = PCA(n_components=3)\n",
    "#     pca.fit(centered)\n",
    "#     normal_vector = pca.components_[-1]  # Last component corresponds to the direction with least variance\n",
    "\n",
    "#     # Step 4: Compute the 'd' value using plane equation: n·x + d = 0 → d = -n·x̄\n",
    "#     d = -np.dot(normal_vector[:3], mean_point[:3])\n",
    "\n",
    "#     return normal_vector[:3], d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpf_estimate_plane(points):\n",
    "    # pts = np.array(points)\n",
    "    mean = np.mean(points, axis=0)\n",
    "    centered = points - mean\n",
    "\n",
    "    cov = np.cov(centered.T)\n",
    "    _, _, vh = np.linalg.svd(cov)\n",
    "    normal = vh[-1]\n",
    "\n",
    "    d = -np.dot(normal, mean)\n",
    "    return (normal, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpf_refinement(point_cloud, number_of_lowest_points=1000, threshold_seeds=0.4, plane_distance_threshold=0.2, num_iterations=5):\n",
    "    \"\"\"\n",
    "    Iteratively refine the ground plane estimation using seed points and distance threshold.\n",
    "    \n",
    "    Args:\n",
    "        point_cloud (np.ndarray): N x 3 or N x D array of input point cloud.\n",
    "        initial_seed_indices (np.ndarray): Indices of initial ground seed points.\n",
    "        distance_threshold (float): Max allowed point-to-plane distance to be considered ground.\n",
    "        num_iterations (int): Number of iterations for refinement loop.\n",
    "    \n",
    "    Returns:\n",
    "        ground_indices (np.ndarray): Indices of points classified as ground.\n",
    "        non_ground_indices (np.ndarray): Indices of points classified as non-ground.\n",
    "    \"\"\"\n",
    "\n",
    "    xyz_point_cloud = point_cloud[:, :3]\n",
    "    seed_indices = gpf_extract_initial_seeds_ids(xyz_point_cloud, number_of_lowest_points, threshold_seeds)\n",
    "\n",
    "    for _ in range(num_iterations):\n",
    "        # Step 1: Estimate plane from current seed points\n",
    "        normal_vector, d = gpf_estimate_plane(xyz_point_cloud[seed_indices])\n",
    "\n",
    "        # Step 2: Compute point-to-plane distances\n",
    "        numerator = np.abs(np.dot(xyz_point_cloud, normal_vector) + d)\n",
    "        denominator = np.linalg.norm(normal_vector)\n",
    "        distances = numerator / denominator\n",
    "\n",
    "        # Step 3: Classify points as ground or non-ground\n",
    "        is_ground = distances < plane_distance_threshold\n",
    "\n",
    "        # Step 4: Update seed indices with the new ground points\n",
    "        seed_indices = np.where(is_ground)[0]\n",
    "\n",
    "    # Final classification after last iteration\n",
    "    ground_indices = np.where(is_ground)[0]\n",
    "    point_cloud[ground_indices, 4] = 9\n",
    "\n",
    "    return point_cloud, normal_vector, d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scan Line Run (SLR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_connected_components(points, eps=0.5, min_samples=1):\n",
    "    \"\"\"\n",
    "    Cluster points into connected components using radius-based neighborhood and Union-Find.\n",
    "\n",
    "    This method groups spatially connected points into clusters. It is a simplified version \n",
    "    of DBSCAN without density-based core points — it only relies on spatial proximity and \n",
    "    minimum cluster size filtering.\n",
    "\n",
    "    Args:\n",
    "        points (np.ndarray): N x 3 (or N x D) array of point coordinates.\n",
    "        eps (float): Maximum distance between neighbors (radius in meters).\n",
    "        min_samples (int): Minimum number of points to form a valid cluster.\n",
    "\n",
    "    Returns:\n",
    "        final_labels (np.ndarray): Cluster labels for each point (-1 for outliers).\n",
    "    \"\"\"\n",
    "    # Step 1: Build neighborhood graph using radius search\n",
    "    neighbors = NearestNeighbors(radius=eps).fit(points)\n",
    "    adjacency = neighbors.radius_neighbors_graph(points, mode='connectivity').tocoo()\n",
    "\n",
    "    # Step 2: Union-Find (Disjoint Set) initialization\n",
    "    parent = np.arange(len(points))\n",
    "\n",
    "    def find(i):\n",
    "        # Path compression\n",
    "        while parent[i] != i:\n",
    "            parent[i] = parent[parent[i]]\n",
    "            i = parent[i]\n",
    "        return i\n",
    "\n",
    "    def union(i, j):\n",
    "        # Union by parent\n",
    "        root_i, root_j = find(i), find(j)\n",
    "        if root_i != root_j:\n",
    "            parent[root_i] = root_j\n",
    "\n",
    "    # Step 3: Connect all neighbors\n",
    "    for i, j in zip(adjacency.row, adjacency.col):\n",
    "        if i != j:\n",
    "            union(i, j)\n",
    "\n",
    "    # Step 4: Assign cluster labels\n",
    "    labels = np.array([find(i) for i in range(len(points))])\n",
    "\n",
    "    # Step 5: Filter small clusters (outliers)\n",
    "    unique_labels, counts = np.unique(labels, return_counts=True)\n",
    "    valid_labels = unique_labels[counts >= min_samples]\n",
    "    mask = np.isin(labels, valid_labels)\n",
    "\n",
    "    # Step 6: Remap valid clusters to consecutive indices, mark outliers as -1\n",
    "    final_labels = -np.ones_like(labels)\n",
    "    final_labels[mask] = np.searchsorted(valid_labels, labels[mask])\n",
    "    return final_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_non_ground_clusters(point_cloud, eps=0.5, min_pts=1):\n",
    "    \"\"\"\n",
    "    Apply spatial clustering to points not classified as ground.\n",
    "\n",
    "    This function filters out ground points (using provided indices),\n",
    "    and applies connected components clustering to the remaining non-ground points.\n",
    "\n",
    "    Args:\n",
    "        point_cloud (np.ndarray): N x 4 array with [x, y, z, class] per point.\n",
    "        ground_indices (np.ndarray): Indices of points classified as ground.\n",
    "\n",
    "    Returns:\n",
    "        cluster_labels (np.ndarray): Cluster label for each non-ground point (-1 for noise).\n",
    "        non_ground_indices (np.ndarray): Indices of the non-ground points in the original array.\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 1: Identify non-ground point indices\n",
    "    non_ground_indices = np.where(point_cloud[:, 4] != 9)[0]\n",
    "\n",
    "    # Step 2: Extract non-ground points for clustering\n",
    "    non_ground_points = point_cloud[non_ground_indices, :3]  # Use only XYZ for clustering\n",
    "\n",
    "    # Step 3: Apply spatial clustering to non-ground points\n",
    "    cluster_labels = cluster_connected_components(non_ground_points, eps=eps, min_samples=min_pts)\n",
    "\n",
    "    point_cloud[non_ground_indices, 4] = cluster_labels\n",
    "\n",
    "    return point_cloud\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizer for Point Clouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_clusters(\n",
    "    point_cloud,\n",
    "    non_ground_indices=None,\n",
    "    cluster_labels=None,\n",
    "    point_size=0.03,\n",
    "    show_ground=True,\n",
    "    show_clusters=True,\n",
    "    show_unlabeled=True,\n",
    "    show_plane=False,\n",
    "    show_true_label=False  # New parameter to toggle label source\n",
    "):\n",
    "    \"\"\"\n",
    "    Visualize a point cloud with consistent class colors using pptk.\n",
    "\n",
    "    Args:\n",
    "        point_cloud (np.ndarray): N x 7 array with [x, y, z, reflectance, gt_label, pred_label, ...].\n",
    "        non_ground_indices (np.ndarray): Indices of non-ground points.\n",
    "        cluster_labels (np.ndarray): Cluster labels for non-ground points (-1 = noise).\n",
    "        point_size (float): Size of points in viewer.\n",
    "        show_ground (bool): Show ground points (label == 1).\n",
    "        show_clusters (bool): Show clustered non-ground points.\n",
    "        show_unlabeled (bool): Show points with label == 0.\n",
    "        show_true_label (bool): Use ground truth labels (col 5) or predicted (col 6).\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Copy the point cloud to avoid modifying original\n",
    "    pc = np.copy(point_cloud)\n",
    "\n",
    "    # Column index to use for visualization\n",
    "    label_col = 3 if show_true_label else 4\n",
    "\n",
    "    # Assign predicted cluster labels if applicable\n",
    "    if cluster_labels is not None and non_ground_indices is not None and show_clusters:\n",
    "        for cluster_id in np.unique(cluster_labels):\n",
    "            if cluster_id == -1:\n",
    "                continue  # skip noise\n",
    "            cluster_point_ids = non_ground_indices[cluster_labels == cluster_id]\n",
    "            pc[cluster_point_ids, label_col] = cluster_id\n",
    "\n",
    "    # Create mask for points to visualize\n",
    "    show_mask = np.full(pc.shape[0], False)\n",
    "    if show_plane:\n",
    "        show_mask |= (pc[:, label_col] == -1)\n",
    "    if show_unlabeled:\n",
    "        show_mask |= (pc[:, label_col] == 0)\n",
    "    if show_ground:\n",
    "        show_mask |= (pc[:, label_col] == 1)\n",
    "    if show_clusters:\n",
    "        show_mask |= (pc[:, label_col] > 1)\n",
    "\n",
    "    # Extract visible points and labels\n",
    "    xyz = pc[show_mask, :3]\n",
    "    class_labels = pc[show_mask, label_col].astype(int)\n",
    "\n",
    "    # Fixed color palette for known classes\n",
    "    fixed_colors = {\n",
    "        -1: [255, 255, 255],\n",
    "        0: [0, 0, 0],\n",
    "        1: [245, 150, 100],\n",
    "        2: [245, 230, 100],\n",
    "        3: [150, 60, 30],\n",
    "        4: [180, 30, 80],\n",
    "        5: [250, 80, 100],\n",
    "        6: [30, 30, 255],\n",
    "        7: [200, 40, 255],\n",
    "        8: [90, 30, 150],\n",
    "        9: [255, 0, 255],\n",
    "        10: [255, 150, 255],\n",
    "        11: [75, 0, 75],\n",
    "        12: [75, 0, 175],\n",
    "        13: [0, 200, 255],\n",
    "        14: [50, 120, 255],\n",
    "        15: [0, 175, 0],\n",
    "        16: [0, 60, 135],\n",
    "        17: [80, 240, 150],\n",
    "        18: [150, 240, 255],\n",
    "        19: [0, 0, 255],\n",
    "    }\n",
    "    # Convert BGR to RGB and normalize\n",
    "    fixed_colors_rgb_normalized = {\n",
    "        label: [bgr[2]/255.0, bgr[1]/255.0, bgr[0]/255.0]  # [R, G, B]\n",
    "        for label, bgr in fixed_colors.items()\n",
    "    }\n",
    "\n",
    "    # Re-map visible class labels to continuous indices for pptk\n",
    "    unique_labels = np.unique(class_labels)\n",
    "    label_to_index = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "    mapped_labels = np.array([label_to_index[label] for label in class_labels])\n",
    "\n",
    "    # Build color map\n",
    "    color_map = []\n",
    "    for label in unique_labels:\n",
    "        if label in fixed_colors_rgb_normalized:\n",
    "            color_map.append(fixed_colors_rgb_normalized[label])\n",
    "        else:\n",
    "            np.random.seed(label)  # keep color stable per class\n",
    "            color_map.append(np.random.rand(3))  # fallback\n",
    "\n",
    "    # Launch viewer\n",
    "    viewer = pptk.viewer(xyz, mapped_labels)\n",
    "    viewer.set(point_size=point_size)\n",
    "    viewer.color_map(color_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate plane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_plane_points(normal, d, center, size=10, resolution=0.5):\n",
    "    \"\"\"\n",
    "    Gera pontos em um plano com base na equação ax + by + cz + d = 0\n",
    "\n",
    "    Args:\n",
    "        normal (np.ndarray): vetor normal (a, b, c)\n",
    "        d (float): distância do plano\n",
    "        center (np.ndarray): centro do plano (x, y, z)\n",
    "        size (float): tamanho do plano\n",
    "        resolution (float): espaçamento entre os pontos\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Mx3 array de pontos do plano\n",
    "    \"\"\"\n",
    "    a = normal[0]\n",
    "    b = normal[1]\n",
    "    c = normal[2]\n",
    "    # Gera grid ao redor do centro\n",
    "    x_vals = np.arange(center[0] - size, center[0] + size, resolution)\n",
    "    y_vals = np.arange(center[1] - size, center[1] + size, resolution)\n",
    "    xx, yy = np.meshgrid(x_vals, y_vals)\n",
    "    \n",
    "    # Equação do plano: ax + by + cz + d = 0  ->  z = (-d - ax - by)/c\n",
    "    zz = (-d - a * xx - b * yy) / c\n",
    "\n",
    "    xyz = np.stack((xx, yy, zz), axis=-1).reshape(-1, 3)\n",
    "    l1 = np.full((xyz.shape[0], 1), -1, dtype=np.float32)\n",
    "    l2 = np.full((xyz.shape[0], 1), -1, dtype=np.float32)\n",
    "    # alterei aqui -----------------------------------\n",
    "    # alterei aqui -----------------------------------\n",
    "    # alterei aqui -----------------------------------\n",
    "    l3 = np.full((xyz.shape[0], 1), -1, dtype=np.float32)\n",
    "\n",
    "    plane_5d = np.hstack((xyz, l1, l2, l3))\n",
    "    # plane_5d = np.hstack((xyz, l1, l2))\n",
    "    return plane_5d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(self, data_path, split='train'):\n",
    "        self.data_path = Path(data_path)\n",
    "        self.yaml_path = self.data_path/'semantic-kitti.yaml'\n",
    "        self.velodynes_path = self.data_path/'data_odometry_velodyne/dataset/sequences'\n",
    "        self.labels_path = self.data_path/'data_odometry_labels/dataset/sequences'\n",
    "\n",
    "        self.is_test = (split == 'test')\n",
    "\n",
    "        # Load YAML metadata\n",
    "        with open(self.yaml_path, 'r') as file:\n",
    "            metadata = yaml.safe_load(file)\n",
    "\n",
    "        self.sequences = metadata['split'][split]\n",
    "        self.learning_map = metadata['learning_map']\n",
    "\n",
    "        max_key = max(self.learning_map.keys())\n",
    "\n",
    "        self.learning_map_np = np.zeros((max_key + 1,), dtype=np.uint32)\n",
    "        for k, v in self.learning_map.items():\n",
    "            self.learning_map_np[k] = v\n",
    "\n",
    "        # List all frames\n",
    "        self.frame_paths = []\n",
    "        for sequence in self.sequences:\n",
    "            sequence = f\"{int(sequence):02d}\"\n",
    "            velo_files = sorted((self.velodynes_path/sequence/'velodyne').glob('*.bin'))\n",
    "            for file in velo_files:\n",
    "                self.frame_paths.append((sequence, file.stem))\n",
    "\n",
    "    def __get_scan_ids_from_order__(self, point_cloud, n_scan=64):\n",
    "        total_points = point_cloud.shape[0]\n",
    "        scan_ids = np.arange(total_points) // (total_points // n_scan)\n",
    "        return scan_ids\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.frame_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        seq, frame_id = self.frame_paths[idx]\n",
    "\n",
    "        velodyne_path = self.velodynes_path/seq/'velodyne'/f\"{frame_id}.bin\"\n",
    "        label_path = self.labels_path/seq/'labels'/f\"{frame_id}.label\"\n",
    "\n",
    "        # Load point cloud\n",
    "        with open(velodyne_path, 'rb') as file:\n",
    "            point_cloud = np.fromfile(file, dtype=np.float32).reshape(-1, 4)[:, :3]\n",
    "\n",
    "        # Load labels\n",
    "        label = None\n",
    "        mask = None\n",
    "        if not self.is_test and label_path.exists():\n",
    "            with open(label_path, 'rb') as file:\n",
    "                label = np.fromfile(file, dtype=np.uint32) & 0xFFFF\n",
    "            label = self.learning_map_np[label]\n",
    "            mask = label != 0\n",
    "        else:\n",
    "            label = np.zeros(point_cloud.shape[0], dtype=np.uint32)\n",
    "            mask = np.ones(point_cloud.shape[0], dtype=bool)\n",
    "\n",
    "        item = {\n",
    "            'point_cloud': point_cloud,\n",
    "            'label': label,\n",
    "            'mask': mask.astype(bool)\n",
    "        }\n",
    "\n",
    "        # alteracao aqui ------------------------------------------\n",
    "        # alteracao aqui ------------------------------------------\n",
    "        # alteracao aqui ------------------------------------------\n",
    "        scan_ids = self.__get_scan_ids_from_order__(point_cloud, n_scan=64)\n",
    "        scan_ids = scan_ids.reshape(-1, 1)\n",
    "        point_cloud_with_label = np.hstack((point_cloud, label.reshape(-1, 1), np.zeros((point_cloud.shape[0], 1)), scan_ids))\n",
    "\n",
    "        # alteracao aqui ------------------------------------------\n",
    "        # descomentar tudo abaixo ------------------------------------------\n",
    "        # ---------------------------------------------------------------\n",
    "        # point_cloud_with_label = np.hstack((point_cloud, label.reshape(-1, 1)))\n",
    "        # adicionando a coluna para ser a coluna de labels gerada pelo algoritmo\n",
    "        # zeros_column = np.zeros((point_cloud.shape[0], 1), dtype=np.float32)\n",
    "        # point_cloud_with_label = np.hstack((point_cloud_with_label, zeros_column))\n",
    "\n",
    "        return point_cloud_with_label, item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(pcd_file):\n",
    "    \"\"\"\n",
    "    Process a single point cloud file using GPF + optional clustering.\n",
    "    Visualizes result using pptk.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load PCD file\n",
    "    point_cloud_from_path = pypcd.PointCloud.from_path(pcd_file)\n",
    "\n",
    "    # Create Nx4 array: x, y, z, class\n",
    "    point_cloud = np.stack((\n",
    "        point_cloud_from_path.pc_data['x'],\n",
    "        point_cloud_from_path.pc_data['y'],\n",
    "        point_cloud_from_path.pc_data['z'],\n",
    "        np.zeros((point_cloud_from_path.pc_data.shape[0])), # default class 0\n",
    "        np.zeros((point_cloud_from_path.pc_data.shape[0])),\n",
    "    ), axis=1)\n",
    "\n",
    "    # Step 1: Extract ground seeds using GPF\n",
    "\n",
    "    # Step 2: Refine ground segmentation\n",
    "    point_cloud, normal, d = gpf_refinement(point_cloud, \n",
    "                                        number_of_lowest_points=N_LPR, \n",
    "                                        threshold_seeds=TH_SEEDS, \n",
    "                                        plane_distance_threshold=TH_DISTANCE_FROM_PLANE,\n",
    "                                        num_iterations=N_ITERATIONS)\n",
    "\n",
    "    # Mark ground points with class = 1\n",
    "    # point_cloud[ground_ids, 3] = 1\n",
    "\n",
    "    # Step 3: Cluster non-ground points (SLR-style)\n",
    "    # pc, cluster_labels, non_ground_ids = process_non_ground_clusters(point_cloud, non_ground_ids, eps=0.5, min_pts=1)\n",
    "\n",
    "    center = point_cloud[:, :3].mean(axis=0)\n",
    "    plane = generate_plane_points(normal, d, center)\n",
    "    point_cloud = np.vstack((point_cloud, plane))\n",
    "\n",
    "    # Step 4: Visualize result\n",
    "    visualize_clusters(point_cloud, show_plane=True)\n",
    "\n",
    "# point_cloud = process_non_ground_clusters(point_cloud, eps=0.5, min_pts=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize_clusters(point_cloud, non_ground_ids, cluster_labels)\n",
    "# cluster_labels.shape, cluster_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of lowest points to use for initial ground seed estimation (LPR)\n",
    "N_LPR = 2000\n",
    "\n",
    "# Number of iterations for ground plane refinement\n",
    "N_ITERATIONS = 5\n",
    "\n",
    "# Height threshold above the LPR to select initial seed points\n",
    "TH_SEEDS = 0.4\n",
    "\n",
    "# Distance threshold to classify a point as ground\n",
    "TH_DISTANCE_FROM_PLANE = 0.2\n",
    "\n",
    "RADIUS_FOR_NEAREST_NEIGHBORS = 0.5\n",
    "\n",
    "MIN_POINTS_FOR_CLUSTER = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset('../datasets/semantic-kitti-data')\n",
    "point_cloud, item = dataset[0]\n",
    "\n",
    "point_cloud, normal, d = gpf_refinement(point_cloud, \n",
    "                                        number_of_lowest_points=N_LPR, \n",
    "                                        threshold_seeds=TH_SEEDS, \n",
    "                                        plane_distance_threshold=TH_DISTANCE_FROM_PLANE,\n",
    "                                        num_iterations=N_ITERATIONS)\n",
    "\n",
    "\n",
    "point_cloud = process_non_ground_clusters(point_cloud, eps=RADIUS_FOR_NEAREST_NEIGHBORS, min_pts=MIN_POINTS_FOR_CLUSTER)\n",
    "\n",
    "center = point_cloud[:, :3].mean(axis=0)\n",
    "plane = generate_plane_points(normal, d, center)\n",
    "point_cloud = np.vstack((point_cloud, plane))\n",
    "\n",
    "visualize_clusters(point_cloud, show_plane=True)\n",
    "\n",
    "np.save('frame_segmentado_slr_diferente.npy', point_cloud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pcd_file = './pointclouds/1504941060.199916000.pcd'\n",
    "\n",
    "# point_cloud_from_path = pypcd.PointCloud.from_path(pcd_file)\n",
    "\n",
    "# point_cloud = np.stack((point_cloud_from_path.pc_data['x'], \n",
    "#                         point_cloud_from_path.pc_data['y'], \n",
    "#                         point_cloud_from_path.pc_data['z'], \n",
    "#                         np.zeros((point_cloud_from_path.pc_data.shape[0])),\n",
    "#                         np.zeros((point_cloud_from_path.pc_data.shape[0])),\n",
    "#                         np.zeros((point_cloud_from_path.pc_data.shape[0])),\n",
    "#                         ), \n",
    "#                         axis=1)\n",
    "\n",
    "# point_cloud, normal, d = gpf_refinement(point_cloud, \n",
    "#                                         number_of_lowest_points=N_LPR, \n",
    "#                                         threshold_seeds=TH_SEEDS, \n",
    "#                                         plane_distance_threshold=TH_DISTANCE_FROM_PLANE,\n",
    "#                                         num_iterations=N_ITERATIONS)\n",
    "\n",
    "# point_cloud = process_non_ground_clusters(point_cloud, eps=0.5, min_pts=1)\n",
    "\n",
    "# center = point_cloud[:, :3].mean(axis=0)\n",
    "# plane = generate_plane_points(normal, d, center)\n",
    "# point_cloud = np.vstack((point_cloud, plane))\n",
    "\n",
    "# visualize_clusters(point_cloud, show_plane=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ground_plane_fitting(point_cloud, N_ITERATIONS, NLPR, Thseeds, Thdist):\n",
    "    xyz_point_cloud = point_cloud[:, :3]\n",
    "    segments = split_into_segments(xyz_point_cloud, N_ITERATIONS)\n",
    "    is_ground_list = []  # Lista que vai juntar os pedaços da máscara\n",
    "\n",
    "    for seg in segments:\n",
    "        is_ground_mask, plane_model = gpf_single_segment(seg, NLPR, Thseeds, Thdist)\n",
    "        is_ground_list.append(is_ground_mask)\n",
    "    \n",
    "    is_ground = np.concatenate(is_ground_list)\n",
    "    ground_indices = np.where(is_ground)[0]\n",
    "    print('is:', is_ground.shape, is_ground)\n",
    "    print('pc:', ground_indices.shape, ground_indices, ground_indices.max())\n",
    "    point_cloud[ground_indices, 4] = 9\n",
    "    \n",
    "    return point_cloud, plane_model\n",
    "\n",
    "def gpf_single_segment(point_cloud, NLPR, Thseeds, Thdist):\n",
    "    seeds = extract_initial_seeds(point_cloud, NLPR, Thseeds)\n",
    "    plane_model = estimate_plane(seeds)\n",
    "    normal_vector, d = plane_model\n",
    "\n",
    "    numerator = np.abs(np.dot(point_cloud, normal_vector) + d)\n",
    "    denominator = np.linalg.norm(normal_vector)\n",
    "    distances = numerator / denominator\n",
    "\n",
    "    # Step 3: Classify points as ground or non-ground\n",
    "    is_ground = distances < Thdist \n",
    "\n",
    "    return is_ground, plane_model\n",
    "\n",
    "def extract_initial_seeds(points, NLPR, Thseeds):\n",
    "    sorted_points = sorted(points, key=lambda p: p[2])  # ordena por altura (z)\n",
    "\n",
    "    lpr = np.mean(sorted_points[:NLPR], axis=0)\n",
    "    seeds = [p for p in points if p[2] < lpr[2] + Thseeds]\n",
    "\n",
    "    return seeds\n",
    "\n",
    "def estimate_plane(points):\n",
    "    pts = np.array(points)\n",
    "    mean = np.mean(pts, axis=0)\n",
    "    centered = pts - mean\n",
    "\n",
    "    cov = np.cov(centered.T)\n",
    "    _, _, vh = np.linalg.svd(cov)\n",
    "    normal = vh[-1]\n",
    "\n",
    "    d = -np.dot(normal, mean)\n",
    "\n",
    "    return (normal, d)\n",
    "\n",
    "def point_to_plane_distance(point, model):\n",
    "    normal, d = model\n",
    "\n",
    "    return abs(np.dot(normal, point) + d) / np.linalg.norm(normal)\n",
    "\n",
    "def split_into_segments(point_cloud, n_segs):\n",
    "    points = sorted(point_cloud, key=lambda p: p[0])\n",
    "    seg_len = len(points) // n_segs\n",
    "\n",
    "    return [points[i*seg_len:(i+1)*seg_len] for i in range(n_segs)]\n",
    "\n",
    "\n",
    "# Number of lowest points to use for initial ground seed estimation (LPR)\n",
    "N_LPR = 20\n",
    "\n",
    "# Number of iterations for ground plane refinement\n",
    "N_ITERATIONS = 3\n",
    "\n",
    "# Height threshold above the LPR to select initial seed points\n",
    "TH_SEEDS = 0.4\n",
    "\n",
    "# Distance threshold to classify a point as ground\n",
    "TH_DISTANCE_FROM_PLANE = 0.2\n",
    "\n",
    "RADIUS_FOR_NEAREST_NEIGHBORS = 0.5\n",
    "\n",
    "MIN_POINTS_FOR_CLUSTER = 1\n",
    "\n",
    "# dataset = Dataset('../datasets/semantic-kitti-data')\n",
    "# point_cloud, item = dataset[0]\n",
    "\n",
    "# point_cloud, plane_model = ground_plane_fitting(point_cloud, N_ITERATIONS, N_LPR, TH_SEEDS, TH_DISTANCE_FROM_PLANE)\n",
    "# normal, d = plane_model\n",
    "\n",
    "# point_cloud = process_non_ground_clusters(point_cloud, eps=RADIUS_FOR_NEAREST_NEIGHBORS, min_pts=MIN_POINTS_FOR_CLUSTER)\n",
    "\n",
    "# center = point_cloud[:, :3].mean(axis=0)\n",
    "# plane = generate_plane_points(normal, d, center)\n",
    "# point_cloud = np.vstack((point_cloud, plane))\n",
    "\n",
    "# visualize_clusters(point_cloud, show_plane=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scan_line_run_clustering(point_cloud):\n",
    "    points = [p for p in point_cloud if p[4] == 0]\n",
    "    points_index = [i for i, p in enumerate(point_cloud) if p[4] == 0]\n",
    "    points = np.stack(points)\n",
    "    scanlines = group_by_scanline(points)\n",
    "    labels = {}\n",
    "    label_counter = 1\n",
    "    label_equiv = {}\n",
    "\n",
    "    runs_above, label_counter = label_runs(scanlines[0], label_counter)\n",
    "    \n",
    "    for i in range(1, len(scanlines)):\n",
    "        runs_curr, label_counter = label_runs(scanlines[i], label_counter)\n",
    "        update_labels(runs_curr, runs_above, label_equiv)\n",
    "        for pt, lbl in runs_above:\n",
    "            labels[pt] = lbl \n",
    "        runs_above = runs_curr\n",
    "\n",
    "    resolve_label_conflicts(labels, label_equiv)\n",
    "    points = extract_clusters(labels)\n",
    "    pt_to_idx = {tuple(points[i]): points_index[i] for i in range(len(points))}\n",
    "\n",
    "    for pt, idx in pt_to_idx.items():\n",
    "        point_cloud[idx] = np.array(pt, dtype=point_cloud.dtype)\n",
    "\n",
    "    return point_cloud\n",
    "\n",
    "def group_by_scanline(points):\n",
    "    scanlines = defaultdict(list)\n",
    "    for p in points:\n",
    "        scan_id = int(p[5])\n",
    "        scanlines[scan_id].append(p)\n",
    "    return [np.array(scanlines[k]) for k in sorted(scanlines.keys())]\n",
    "\n",
    "def label_runs(scanline, label_counter):\n",
    "    runs = []\n",
    "    current_run = []\n",
    "    for idx, point in enumerate(scanline):\n",
    "        if not current_run or np.linalg.norm(point - scanline[idx-1]) < RADIUS_FOR_NEAREST_NEIGHBORS:\n",
    "            current_run.append(point)\n",
    "        else:\n",
    "            label = label_counter\n",
    "            label_counter += 1\n",
    "            for pt in current_run:\n",
    "                pt_id = tuple(pt)\n",
    "                runs.append((pt_id, label))\n",
    "            current_run = [point]\n",
    "    if current_run:\n",
    "        label = label_counter\n",
    "        label_counter += 1\n",
    "        if label_counter == 9:\n",
    "            label_counter += 1\n",
    "        for pt in current_run:\n",
    "            pt_id = tuple(pt)\n",
    "            runs.append((pt_id, label))\n",
    "    return runs, label_counter\n",
    "\n",
    "def update_labels(current_runs, previous_runs, label_equiv):\n",
    "    prev_dict = dict(previous_runs)\n",
    "    for pt_id, label in current_runs:\n",
    "        nn_label = find_nearest_label(pt_id, prev_dict)\n",
    "        if nn_label:\n",
    "            if label != nn_label:\n",
    "                label_equiv[label] = nn_label\n",
    "\n",
    "def find_nearest_label(pt_id, prev_dict):\n",
    "    pt = np.array(pt_id)\n",
    "    min_dist = float('inf')\n",
    "    nearest = None\n",
    "    for other_pt_id, lbl in prev_dict.items():\n",
    "        dist = np.linalg.norm(pt[:3] - np.array(other_pt_id)[:3])\n",
    "        if dist < Thmerge and dist < min_dist:\n",
    "            min_dist = dist\n",
    "            nearest = lbl\n",
    "    return nearest\n",
    "\n",
    "def resolve_label_conflicts(labels, label_equiv):\n",
    "    for pt_id, lbl in labels.items():\n",
    "        while lbl in label_equiv:\n",
    "            lbl = label_equiv[lbl]\n",
    "        labels[pt_id] = lbl\n",
    "\n",
    "def extract_clusters(labels):\n",
    "    points = []\n",
    "    for pt_id, label in labels.items():\n",
    "        p = np.array(pt_id)\n",
    "        p[4] = label\n",
    "        points.append(p)\n",
    "    return np.stack(points)\n",
    "\n",
    "# Exemplo de ponto: [x, y, z]\n",
    "def get_scan_id(num_points, N_SCAN=64):\n",
    "    # # Exemplo fictício baseado em altura ou ângulo vertical\n",
    "    # return int(point[1] // 1.0)  # ajuste conforme o sensor\n",
    "    #----------------------------------------------------------------------------\n",
    "    # inferencia calculando o angulo\n",
    "    # x, y, z = point[:3]\n",
    "    # r = np.linalg.norm([x, y, z])\n",
    "    # vertical_angle = np.arcsin(z / r) * 180 / np.pi  # em graus\n",
    "    # # Aqui assume-se que o LiDAR tem N_SCAN camadas uniformemente distribuídas entre -24.8 e +2 graus (exemplo Velodyne HDL-64E)\n",
    "    # angle_min = -24.8\n",
    "    # angle_max = 2.0\n",
    "    # scan_id = int(((vertical_angle - angle_min) / (angle_max - angle_min)) * N_SCAN)\n",
    "    # scan_id = np.clip(scan_id, 0, N_SCAN - 1)  # garante que está no intervalo\n",
    "    #----------------------------------------------------------------------------\n",
    "    scan_ids = np.repeat(np.arange(N_SCAN), num_points // N_SCAN)\n",
    "    return scan_ids\n",
    "\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# -------------------------------------------------------------------------\n",
    "# -------------------------------------------------------------------------\n",
    "# -------------------------------------------------------------------------\n",
    "# Number of lowest points to use for initial ground seed estimation (LPR)\n",
    "N_LPR = 2000\n",
    "\n",
    "# Number of iterations for ground plane refinement\n",
    "N_ITERATIONS = 5\n",
    "\n",
    "# Height threshold above the LPR to select initial seed points\n",
    "TH_SEEDS = 0.4\n",
    "\n",
    "# Distance threshold to classify a point as ground\n",
    "TH_DISTANCE_FROM_PLANE = 0.2\n",
    "\n",
    "RADIUS_FOR_NEAREST_NEIGHBORS = 0.5\n",
    "\n",
    "Thmerge = 1.0\n",
    "\n",
    "MIN_POINTS_FOR_CLUSTER = 1\n",
    "\n",
    "dataset = Dataset('../datasets/semantic-kitti-data')\n",
    "point_cloud, item = dataset[0]\n",
    "\n",
    "point_cloud, normal, d = gpf_refinement(point_cloud, \n",
    "                                        number_of_lowest_points=N_LPR, \n",
    "                                        threshold_seeds=TH_SEEDS, \n",
    "                                        plane_distance_threshold=TH_DISTANCE_FROM_PLANE,\n",
    "                                        num_iterations=N_ITERATIONS)\n",
    "\n",
    "# point_cloud = process_non_ground_clusters(point_cloud, eps=RADIUS_FOR_NEAREST_NEIGHBORS, min_pts=MIN_POINTS_FOR_CLUSTER)\n",
    "point_cloud = scan_line_run_clustering(point_cloud)\n",
    "\n",
    "center = point_cloud[:, :3].mean(axis=0)\n",
    "plane = generate_plane_points(normal, d, center)\n",
    "point_cloud = np.vstack((point_cloud, plane))\n",
    "\n",
    "visualize_clusters(point_cloud, show_plane=True)\n",
    "\n",
    "np.save('frame_segmentado_slr_igual.npy', point_cloud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_segmentado_1 = np.load('frame_segmentado_slr_diferente.npy')\n",
    "frame_segmentado_2 = np.load('frame_segmentado_slr_igual.npy')\n",
    "\n",
    "visualize_clusters(frame_segmentado_1, show_true_label=True)\n",
    "visualize_clusters(frame_segmentado_1, show_plane=True)\n",
    "visualize_clusters(frame_segmentado_2, show_plane=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "all the input array dimensions for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 5 and the array at index 1 has size 6",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_175422/4189929831.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./pointclouds/1504941055.292141000.pcd'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./pointclouds/1504941060.199916000.pcd'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_175422/3511028857.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m(pcd_file)\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mcenter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpoint_cloud\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mplane\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_plane_points\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnormal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcenter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0mpoint_cloud\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpoint_cloud\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplane\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;31m# Step 4: Visualize result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mvstack\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/segmentation-point-clouds/lib/python3.7/site-packages/numpy/core/shape_base.py\u001b[0m in \u001b[0;36mvstack\u001b[0;34m(tup)\u001b[0m\n\u001b[1;32m    280\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m         \u001b[0marrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0marrs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 282\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_nx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: all the input array dimensions for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 5 and the array at index 1 has size 6"
     ]
    }
   ],
   "source": [
    "main('./pointclouds/1504941055.292141000.pcd')\n",
    "main('./pointclouds/1504941060.199916000.pcd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_all_pointclouds(folder_path='./pointclouds'):\n",
    "    \"\"\"\n",
    "    Find and process all .pcd files in the specified folder using the main() function.\n",
    "    \"\"\"\n",
    "    pcd_files = sorted([f for f in os.listdir(folder_path) if f.endswith('.pcd')])\n",
    "\n",
    "    for filename in pcd_files:\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        print(f\"\\nProcessing: {filename}\")\n",
    "        try:\n",
    "            main(file_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {filename}: {e}\")\n",
    "\n",
    "# Run the batch\n",
    "# run_all_pointclouds('./pointclouds')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "segmentation-point-clouds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
