{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ground Plane Fitting and Scan Line Run for 3D LiDAR\n",
    "\n",
    "Ground Plane Fitting (GPF) and Naive Baseline for 3D LiDAR Segmentation\n",
    "\n",
    "This notebook implements ground segmentation using the Ground Plane Fitting (GPF) algorithm \n",
    "proposed in:\n",
    "\n",
    "\"Fast Segmentation of 3D Point Clouds: A Paradigm on LiDAR Data for Autonomous Vehicle Applications\"\n",
    "by D. Zermas, I. Izzat, and N. Papanikolopoulos, 2017.\n",
    "\n",
    "The implementation also includes a naive baseline method for comparison, as well as \n",
    "basic clustering and visualization tools."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Third-party imports\n",
    "import numpy as np\n",
    "import yaml\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import KDTree  # Used for spatial clustering\n",
    "\n",
    "# Local application imports\n",
    "# Note: Ubuntu users may need to fix libz.so.1 symlink issue (see: https://github.com/heremaps/pptk/issues/3)\n",
    "import pptk  # To install: pip install pptk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Hyperparameters for Ground Plane Fitting (GPF) ---\n",
    "NUM_LOWEST_POINTS = 2000           # Number of lowest elevation points used to estimate initial ground seed (LPR)\n",
    "NUM_ITERATIONS = 5                 # Number of iterations for plane refinement in GPF\n",
    "SEED_HEIGHT_THRESHOLD = 0.4        # Max height above LPR to consider a point as a ground seed\n",
    "PLANE_DISTANCE_THRESHOLD = 0.2     # Max distance from plane to classify a point as ground\n",
    "\n",
    "# --- Parameters for Scan Line Run (SLR) clustering ---\n",
    "SLR_RUN_DISTANCE_THRESHOLD = 0.2   # Max distance between consecutive points in a scanline to form a run\n",
    "SLR_MERGE_THRESHOLD = 1.0          # Max distance between runs in adjacent scanlines to be considered the same cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Baseline Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_ground_extractor(point_cloud: np.ndarray, num_lowest_points: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Naive ground extraction method (baseline).\n",
    "    \n",
    "    This simple method selects the points with the lowest Z values \n",
    "    and assumes they belong to the ground surface. It does not model \n",
    "    the ground plane and is used as a baseline for comparison against \n",
    "    more robust algorithms like Ground Plane Fitting (GPF).\n",
    "    \n",
    "    Args:\n",
    "        point_cloud (np.ndarray): N x D array of point cloud data.\n",
    "        num_lowest_points (int): Number of points with lowest Z values to classify as ground.\n",
    "    \n",
    "    Returns:\n",
    "        ground_indices (np.ndarray): Indices of the selected ground points.\n",
    "    \"\"\"\n",
    "\n",
    "    # Select indices of points with the lowest Z values\n",
    "    return np.argsort(point_cloud[:, 2])[:num_lowest_points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ground Plane Fitting (GPF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_initial_seed_indices(\n",
    "    point_cloud: np.ndarray, \n",
    "    num_points: int = 1000, \n",
    "    height_threshold: float = 0.4\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Extract initial seed points for ground plane estimation (GPF).\n",
    "    \n",
    "    Args:\n",
    "        point_cloud (np.ndarray): N x 3 array of points (x, y, z).\n",
    "        num_points (int): number of lowest Z points to average as LPR.\n",
    "        height_threshold (float): threshold to select seeds close to LPR height.\n",
    "    \n",
    "    Returns:\n",
    "        seeds_ids (np.ndarray): indices of points selected as initial seeds.\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 1: Sort the point cloud by Z axis (height)\n",
    "    sorted_indices = np.argsort(point_cloud[:, 2])  # Get indices sorted by height\n",
    "    sorted_points = point_cloud[sorted_indices]     # Apply sorting\n",
    "\n",
    "    # Step 2: Compute LPR (Lowest Point Representative)\n",
    "    lpr_height = np.mean(sorted_points[:num_points, 2])\n",
    "\n",
    "    # Step 3: Select point ids that are within threshold distance from LPR\n",
    "    mask = sorted_points[:, 2] < (lpr_height + height_threshold)\n",
    "    return sorted_indices[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_ground_plane(points: np.ndarray) -> \"tuple[np.ndarray, float]\":\n",
    "    \"\"\"\n",
    "    Estimate the ground plane parameters using Singular Value Decomposition (SVD).\n",
    "    \n",
    "    Args:\n",
    "        points (np.ndarray): N x 3 array (x, y, z) of seed points assumed to be on or near the ground.\n",
    "\n",
    "    Returns:\n",
    "        tuple: \n",
    "            - normal (np.ndarray): Normal vector (a, b, c) of the estimated ground plane.\n",
    "            - d (float): Offset term of the estimated plane equation (ax + by + cz + d = 0).\n",
    "    \"\"\"\n",
    "    \n",
    "    # Step 1: Compute centroid of the seed points\n",
    "    centroid  = np.mean(points, axis=0)\n",
    "    centered_points  = points - centroid \n",
    "\n",
    "    # Step 2: Compute the covariance matrix of centered points\n",
    "    covariance_matrix = np.cov(centered_points.T)\n",
    "\n",
    "    # Step 3: Perform SVD on the covariance matrix to extract principal directions\n",
    "    _, _, vh = np.linalg.svd(covariance_matrix)\n",
    "\n",
    "    # Step 4: Normal vector is the direction with smallest variance (last column of V^T)\n",
    "    normal = vh[-1]\n",
    "\n",
    "    # Step 5: Compute plane bias using point-normal form: ax + by + cz + d = 0\n",
    "    d = -np.dot(normal, centroid)\n",
    "\n",
    "    return (normal, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def refine_ground_plane(\n",
    "    point_cloud: np.ndarray,\n",
    "    num_points: int = 1000,\n",
    "    height_threshold: float = 0.4,\n",
    "    distance_threshold: float = 0.2,\n",
    "    num_iterations: int = 5\n",
    ") -> \"tuple[np.ndarray, np.ndarray, float]\":\n",
    "    \"\"\"\n",
    "    Iteratively refine the ground plane estimation using seed points and distance threshold.\n",
    "    \n",
    "    Args:\n",
    "        point_cloud (np.ndarray): Nx6 array [x, y, z, true_label, pred_label, scanline_id].\n",
    "        num_points (int): Number of lowest Z points used to compute the initial ground seed height (LPR).\n",
    "        height_threshold (float): Vertical distance threshold from the LPR used to select initial seed points.\n",
    "        distance_threshold (float): Max allowed point-to-plane distance for a point to be considered ground.\n",
    "        num_iterations (int): Number of iterations to refine the plane and ground classification.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: \n",
    "            - point_cloud (np.ndarray): Nx6 array [x, y, z, true_label, pred_label, scanline_id], input array with ground points labeled.\n",
    "            - normal (np.ndarray): Normal vector (a, b, c) of the estimated ground plane.\n",
    "            - d (float): Offset term of the estimated plane equation (ax + by + cz + d = 0).\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 0: Use only XYZ for plane estimation\n",
    "    xyz = point_cloud[:, :3]\n",
    "\n",
    "    # Step 1: Get initial seed points based on lowest Z values\n",
    "    seed_indices = extract_initial_seed_indices(xyz, num_points, height_threshold)\n",
    "\n",
    "    for _ in range(num_iterations):\n",
    "        # Step 2: Estimate ground plane using current seeds\n",
    "        normal, d = estimate_ground_plane(xyz[seed_indices])\n",
    "\n",
    "        # Step 3: Compute distances from all points to the estimated plane\n",
    "        distances = np.abs(np.dot(xyz, normal) + d) / np.linalg.norm(normal)\n",
    "\n",
    "        # Step 4: Classify as ground if within distance threshold\n",
    "        is_ground = distances < distance_threshold\n",
    "\n",
    "        # Step 5: Update seeds with newly classified ground points\n",
    "        seed_indices = np.where(is_ground)[0]\n",
    "\n",
    "    # Final ground classification using last iteration's result\n",
    "    point_cloud[seed_indices, 4] = 9 # Set label = 9 for ground\n",
    "\n",
    "    return (point_cloud, normal, d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scan Line Run (SLR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_by_scanline(point_cloud: np.ndarray) -> \"list[np.ndarray]\":\n",
    "    \"\"\"\n",
    "    Group points by their scanline index in a vectorized way.\n",
    "\n",
    "    Args:\n",
    "        point_cloud (np.ndarray): N x 6 array [x, y, z, true_label, pred_label, scanline_id].\n",
    "\n",
    "    Returns:\n",
    "        list[np.ndarray]: List of arrays. Each array contains the points (N_i x 6)\n",
    "                          from one scanline, sorted by scanline_id.\n",
    "    \"\"\"\n",
    "    scan_ids = point_cloud[:, 5].astype(int)\n",
    "    unique_ids = np.unique(scan_ids)\n",
    "\n",
    "    return [point_cloud[scan_ids == s_id] for s_id in unique_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_runs(scanline_points: np.ndarray, distance_threshold: float = 0.5) -> \"list[np.ndarray]\":\n",
    "    \"\"\"\n",
    "    Identify runs within a single scanline based on distance between consecutive points.\n",
    "\n",
    "    Args:\n",
    "        scanline_points (np.ndarray): N x 6 array [x, y, z, true_label, pred_label, scanline_id].\n",
    "        distance_threshold (float): Distance threshold to consider two points part of the same run.\n",
    "\n",
    "    Returns:\n",
    "        list[np.ndarray]: List of arrays where each array contains the points of a run.\n",
    "    \"\"\"\n",
    "    num_points = len(scanline_points)\n",
    "    runs = []\n",
    "    current_run_indices = [0]  # start with the index of the first point\n",
    "\n",
    "    for i in range(1, num_points):\n",
    "        dist = np.linalg.norm(scanline_points[i, :3] - scanline_points[i - 1, :3])\n",
    "        if dist < distance_threshold:\n",
    "            current_run_indices.append(i)\n",
    "        else:\n",
    "            runs.append(scanline_points[current_run_indices])\n",
    "            current_run_indices = [i]\n",
    "\n",
    "    # append the last run\n",
    "    runs.append(scanline_points[current_run_indices])\n",
    "\n",
    "    # Check if first and last points are close (circular case)\n",
    "    circular_dist = np.linalg.norm(scanline_points[0, :3] - scanline_points[-1, :3])\n",
    "    # Only merge runs if:\n",
    "    # - the scanline appears to be circular (first and last points are close), and\n",
    "    # - there is more than one run (otherwise merging doesn't make sense)\n",
    "    if circular_dist < distance_threshold and len(runs) > 1:\n",
    "        # Merge last run with the first\n",
    "        runs[0] = np.vstack((runs[-1], runs[0]))\n",
    "        runs.pop()\n",
    "\n",
    "    return runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_labels(\n",
    "    runs_current: \"list[np.ndarray]\",\n",
    "    runs_above: \"list[np.ndarray]\",\n",
    "    label_equivalences: dict,\n",
    "    merge_threshold: float = 1.0\n",
    "):\n",
    "    \"\"\"\n",
    "    Update labels of current scanline runs based on proximity to runs from previous scanline using KDTree.\n",
    "\n",
    "    Args:\n",
    "        runs_current (list[np.ndarray]): List of N x 6 arrays for current scanline runs.\n",
    "        runs_above (list[np.ndarray]): List of N x 6 arrays for previous scanline runs.\n",
    "        label_equivalences (dict): Dictionary of label equivalences.\n",
    "        merge_threshold (float): Maximum distance to consider connection between runs.\n",
    "    \"\"\"\n",
    "    def resolve_label(label: int) -> int:\n",
    "        \"\"\"Find the final label by following the equivalence chain.\"\"\"\n",
    "        while label != label_equivalences[label]:\n",
    "            label = label_equivalences[label]\n",
    "        return label\n",
    "\n",
    "    global_label_counter = max(label_equivalences.values()) + 1\n",
    "\n",
    "    points_above = np.vstack(runs_above)\n",
    "    tree_above = KDTree(points_above[:, :3])  # use only x, y, z\n",
    "\n",
    "    for run in runs_current:\n",
    "        neighbor_labels = set()\n",
    "\n",
    "        # Check nearest neighbor of each point in current run\n",
    "        dists, indices = tree_above.query(run[:, :3], k=1)\n",
    "        for dist, idx in zip(dists[:, 0], indices[:, 0]):\n",
    "            if dist < merge_threshold:\n",
    "                neighbor_label = points_above[idx, 4]\n",
    "                resolved_label = resolve_label(neighbor_label)\n",
    "                neighbor_labels.add(resolved_label)\n",
    "\n",
    "        if not neighbor_labels:\n",
    "            # No close neighbors → assign new label\n",
    "            while global_label_counter == 9 or global_label_counter in label_equivalences:\n",
    "                global_label_counter += 1\n",
    "            run[:, 4] = global_label_counter\n",
    "            label_equivalences[global_label_counter] = global_label_counter\n",
    "        else:\n",
    "            # Inherit the smallest label and unify equivalences\n",
    "            min_label = min(neighbor_labels)\n",
    "            run[:, 4] = min_label\n",
    "            for lbl in neighbor_labels:\n",
    "                label_equivalences[lbl] = min_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_clusters(scanlines: \"list[np.ndarray]\", label_equivalences: dict) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Apply resolved labels to all points and return a unified point cloud.\n",
    "\n",
    "    Args:\n",
    "        scanlines (list[np.ndarray]): List of N x 6 arrays for each scanline.\n",
    "        label_equivalences (dict): Dictionary of final label equivalences.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: N x 6 array with updated labels in column 4.\n",
    "    \"\"\"\n",
    "    non_ground_points = np.vstack(scanlines)\n",
    "\n",
    "    for idx in range(0, len(non_ground_points)):\n",
    "        non_ground_points[idx][4] = label_equivalences[non_ground_points[idx][4]]\n",
    "\n",
    "    return non_ground_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scan_line_run_clustering(\n",
    "    point_cloud: np.ndarray, \n",
    "    distance_threshold: float = 0.5, \n",
    "    merge_threshold: float = 1.0\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Perform scan line run clustering on non-ground points (predicted_label == 0).\n",
    "\n",
    "    This function detects connected components (runs) within scanlines, propagates\n",
    "    and merges labels across scanlines, and assigns final labels to each point.\n",
    "\n",
    "    Args:\n",
    "        point_cloud (np.ndarray): N x 6 array [x, y, z, true_label, predicted_label, scanline_index].\n",
    "        distance_threshold (float): Distance threshold to consider two points part of the same run.\n",
    "        merge_threshold (float): Maximum distance to consider connection between runs.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Point cloud with updated predicted labels (column 4).\n",
    "    \"\"\"\n",
    "    label_counter = 0\n",
    "    label_equivalences = {}\n",
    "\n",
    "    # Filter non-ground points (predicted_label == 0)\n",
    "    non_ground_mask = point_cloud[:, 4] == 0\n",
    "    non_ground_points = point_cloud[non_ground_mask]\n",
    "    ground_points = point_cloud[~non_ground_mask]\n",
    "\n",
    "    # Group points into scanlines\n",
    "    scanlines = group_by_scanline(non_ground_points)\n",
    "\n",
    "    # Initialize clustering with the first scanline\n",
    "    runs_above = find_runs(scanlines[0], distance_threshold)\n",
    "    for runs in runs_above:\n",
    "        label_counter += 1\n",
    "        if label_counter == 9:  # reserve label 9 for ground\n",
    "            label_counter += 1\n",
    "        runs[:, 4] = label_counter\n",
    "        label_equivalences[label_counter] = label_counter\n",
    "\n",
    "    scanlines[0] = np.vstack(runs_above)\n",
    "        \n",
    "    # Propagate labels through remaining scanlines\n",
    "    for i in range(1, len(scanlines)):\n",
    "        runs_current = find_runs(scanlines[i], distance_threshold)\n",
    "        update_labels(runs_current, runs_above, label_equivalences, merge_threshold)\n",
    "\n",
    "        scanlines[i] = np.vstack(runs_current)\n",
    "        runs_above = runs_current\n",
    "\n",
    "    clustered_points = extract_clusters(scanlines, label_equivalences)\n",
    "    return np.vstack((clustered_points, ground_points))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(self, data_path: str, split: str = 'train') -> None:\n",
    "        \"\"\"\n",
    "        Initialize dataset loader.\n",
    "\n",
    "        Args:\n",
    "            data_path (str or Path): Base path to the SemanticKITTI dataset.\n",
    "            split (str): Dataset split to use ('train', 'valid', or 'test').\n",
    "        \"\"\"\n",
    "        self.data_path: Path = Path(data_path)\n",
    "        self.split: str = split\n",
    "        self.is_test: bool = split == 'test'\n",
    "\n",
    "        # Paths to YAML config and data folders\n",
    "        self.yaml_path: Path = self.data_path / 'semantic-kitti.yaml'\n",
    "        self.velodynes_path: Path = self.data_path / 'data_odometry_velodyne/dataset/sequences'\n",
    "        self.labels_path: Path = self.data_path / 'data_odometry_labels/dataset/sequences'\n",
    "\n",
    "        # Load dataset metadata and label mappings\n",
    "        with open(self.yaml_path, 'r') as file:\n",
    "            metadata: dict = yaml.safe_load(file)\n",
    "\n",
    "        self.sequences: list[int] = metadata['split'][split]\n",
    "        self.learning_map: dict[int, int] = metadata['learning_map']\n",
    "\n",
    "        # Convert label map to numpy for fast lookup\n",
    "        max_label: int = max(self.learning_map.keys())\n",
    "        self.learning_map_np: np.ndarray = np.zeros((max_label + 1,), dtype=np.uint32)\n",
    "        for raw_label, mapped_label in self.learning_map.items():\n",
    "            self.learning_map_np[raw_label] = mapped_label\n",
    "\n",
    "        # Collect all frame paths for selected sequences\n",
    "        self.frame_paths: list[tuple[str, str]] = self._collect_frame_paths()\n",
    "\n",
    "    def _collect_frame_paths(self) -> \"list[tuple[str, str]]\":\n",
    "        \"\"\"Collect all (sequence, frame_id) pairs from the dataset split.\"\"\"\n",
    "        frame_list = []\n",
    "        for seq in self.sequences:\n",
    "            seq_str = f\"{int(seq):02d}\"\n",
    "            seq_velo_path = self.velodynes_path/seq_str/'velodyne'\n",
    "            velo_files = sorted(seq_velo_path.glob('*.bin'))\n",
    "            for file in velo_files:\n",
    "                frame_list.append((seq_str, file.stem))\n",
    "        return frame_list\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"Return number of samples in the dataset split.\"\"\"\n",
    "        return len(self.frame_paths)\n",
    "\n",
    "    def _compute_scanline_ids(self, point_cloud: np.ndarray, n_scans: int = 64) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Approximate scanline indices based on point order.\n",
    "\n",
    "        Args:\n",
    "            point_cloud (np.ndarray): Nx3 array of 3D points.\n",
    "            n_scans (int): Number of LiDAR scanlines (e.g., 64 for HDL-64E).\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Nx1 array with estimated scanline indices (0 to n_scans - 1).\n",
    "        \"\"\"\n",
    "        total_points = point_cloud.shape[0]\n",
    "        scanline_ids = np.floor(np.linspace(0, n_scans, total_points, endpoint=False)).astype(int)\n",
    "        return scanline_ids.reshape(-1, 1)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> \"tuple[np.ndarray, dict[str, np.ndarray]]\":\n",
    "        \"\"\"\n",
    "        Load a sample from the dataset.\n",
    "\n",
    "        Args:\n",
    "            idx (int): Index of the frame to load.\n",
    "\n",
    "        Returns:\n",
    "            tuple:\n",
    "                - point_cloud_with_label (np.ndarray): Nx6 array [x, y, z, true_label, pred_label, scanline_id].\n",
    "                - item_dict (dict): Contains 'point_cloud', 'label', and 'mask'.\n",
    "        \"\"\"\n",
    "\n",
    "        seq, frame_id = self.frame_paths[idx]\n",
    "\n",
    "        # Load point cloud (Nx4), drop reflectance\n",
    "        velodyne_file_path = self.velodynes_path/seq/'velodyne'/f\"{frame_id}.bin\"\n",
    "        with open(velodyne_file_path, 'rb') as file:\n",
    "            point_cloud = np.fromfile(file, dtype=np.float32).reshape(-1, 4)[:, :3]\n",
    "\n",
    "        # Load and map semantic labels\n",
    "        if not self.is_test:\n",
    "            label_file_path = self.labels_path/seq/'labels'/f\"{frame_id}.label\"\n",
    "            if label_file_path.exists():\n",
    "                with open(label_file_path, 'rb') as file:\n",
    "                    raw_labels = np.fromfile(file, dtype=np.uint32) & 0xFFFF\n",
    "                labels = self.learning_map_np[raw_labels]\n",
    "                mask = labels != 0\n",
    "            else:\n",
    "                labels = np.zeros(point_cloud.shape[0], dtype=np.uint32)\n",
    "                mask = np.ones(point_cloud.shape[0], dtype=bool)\n",
    "        else:\n",
    "            labels = np.zeros(point_cloud.shape[0], dtype=np.uint32)\n",
    "            mask = np.ones(point_cloud.shape[0], dtype=bool)\n",
    "\n",
    "        # Estimate scanline indices\n",
    "        scanline_ids = self._compute_scanline_ids(point_cloud)\n",
    "\n",
    "        # Final format: [x, y, z, true_label, predicted_label, scanline_id]\n",
    "        point_cloud_with_label = np.hstack((\n",
    "            point_cloud,\n",
    "            labels.reshape(-1, 1),\n",
    "            np.zeros((point_cloud.shape[0], 1), dtype=np.float32),\n",
    "            scanline_ids\n",
    "        ))\n",
    "\n",
    "        item_dict = {\n",
    "            'point_cloud': point_cloud,\n",
    "            'label': labels,\n",
    "            'mask': mask\n",
    "        }\n",
    "\n",
    "        return point_cloud_with_label, item_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Plane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_plane_points(\n",
    "    point_cloud: np.ndarray, \n",
    "    normal: np.ndarray, \n",
    "    d: float, \n",
    "    size: float = 30, \n",
    "    resolution: float = 0.5\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Generate a grid of 3D points lying on a specified plane, and return them with label placeholders.\n",
    "    The plane is defined by the equation: ax + by + cz + d = 0\n",
    "\n",
    "    Args:\n",
    "        point_cloud (np.ndarray): Nx6 array [x, y, z, true_label, pred_label, scanline_id].\n",
    "        normal (np.ndarray): Plane normal vector [a, b, c].\n",
    "        d (float): Plane offset in the equation ax + by + cz + d = 0.\n",
    "        size (float): Half-length of the plane square grid to generate (in meters).\n",
    "        resolution (float): Spacing between points in the grid.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Mx6 array of points with [x, y, z, label1, label2, label3],\n",
    "                    where the last 3 columns are filled with -1 as placeholders.\n",
    "    \"\"\"\n",
    "\n",
    "    # Compute center of the plane as the centroid of the point cloud\n",
    "    center = point_cloud[:, :3].mean(axis=0)\n",
    "    a, b, c = normal\n",
    "\n",
    "    # Create a mesh grid around the center point in the XY plane\n",
    "    x_vals = np.arange(center[0] - size, center[0] + size, resolution)\n",
    "    y_vals = np.arange(center[1] - size, center[1] + size, resolution)\n",
    "    xx, yy = np.meshgrid(x_vals, y_vals)\n",
    "    \n",
    "    # Solve for z using the plane equation: ax + by + cz + d = 0 => z = (-d - ax - by)/c\n",
    "    zz = (-d - a * xx - b * yy) / c\n",
    "\n",
    "    # Stack into N x 3 array of [x, y, z]\n",
    "    xyz = np.stack((xx, yy, zz), axis=-1).reshape(-1, 3)\n",
    "\n",
    "    # Create label columns filled with -1 as placeholders (e.g., for plane visualization)\n",
    "    labels = np.full((xyz.shape[0], 3), -1, dtype=np.float32)\n",
    "\n",
    "    # Concatenate coordinates and labels into a final N x 6 array\n",
    "    return np.hstack((xyz, labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizer for Point Clouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PointCloudVisualizer:\n",
    "    def __init__(self, point_size: float = 0.03):\n",
    "        \"\"\"\n",
    "        A visualizer class for rendering point clouds using pptk with color-coded semantic labels.\n",
    "\n",
    "        Args:\n",
    "            point_size (float): Default size of points in the pptk viewer.\n",
    "        \"\"\"\n",
    "        self.point_size: float = point_size\n",
    "        self.fixed_colors_rgb: \"dict[int, list[float]]\" = self._get_fixed_colors_rgb()\n",
    "\n",
    "    def _get_fixed_colors_rgb(self) -> \"dict[int, list[float]]\":\n",
    "        \"\"\"\n",
    "        Returns a dictionary mapping label ids to normalized RGB colors.\n",
    "        The colors follow the SemanticKITTI color convention (converted from BGR to RGB).\n",
    "\n",
    "        Returns:\n",
    "            dict: {label: [R, G, B]} with values in [0, 1].\n",
    "        \"\"\"\n",
    "        fixed_colors = {\n",
    "            -1: [255, 255, 255],  # plane\n",
    "             0: [0, 0, 0],        # unlabeled\n",
    "             1: [245, 150, 100],  # car\n",
    "             2: [245, 230, 100],  # bicycle\n",
    "             3: [150, 60, 30],    # motorcycle\n",
    "             4: [180, 30, 80],    # truck\n",
    "             5: [250, 80, 100],   # other-vehicle\n",
    "             6: [30, 30, 255],    # person\n",
    "             7: [200, 40, 255],   # bicyclist\n",
    "             8: [90, 30, 150],    # motorcyclist\n",
    "             9: [255, 0, 255],    # road\n",
    "            10: [255, 150, 255],  # parking\n",
    "            11: [75, 0, 75],      # sidewalk\n",
    "            12: [75, 0, 175],     # other-ground\n",
    "            13: [0, 200, 255],    # building\n",
    "            14: [50, 120, 255],   # fence\n",
    "            15: [0, 175, 0],      # vegetation\n",
    "            16: [0, 60, 135],     # trunk\n",
    "            17: [80, 240, 150],   # terrain\n",
    "            18: [150, 240, 255],  # pole\n",
    "            19: [0, 0, 255],      # traffic-sign\n",
    "        }\n",
    "        return {label: [c / 255.0 for c in reversed(rgb)] for label, rgb in fixed_colors.items()}\n",
    "\n",
    "    def _get_color_map(self, unique_labels: np.ndarray) -> \"list[list[float]]\":\n",
    "        \"\"\"\n",
    "        Generate a color map for a set of unique labels.\n",
    "        If a label is not found in the fixed colors, generate a consistent random color.\n",
    "\n",
    "        Args:\n",
    "            unique_labels (np.ndarray): Unique label ids to assign colors to.\n",
    "\n",
    "        Returns:\n",
    "            list: List of RGB color triplets.\n",
    "        \"\"\"\n",
    "        color_map = []\n",
    "        for label in unique_labels:\n",
    "            if label in self.fixed_colors_rgb:\n",
    "                color_map.append(self.fixed_colors_rgb[label])\n",
    "            else:\n",
    "                np.random.seed(label)  # deterministic color per label\n",
    "                color_map.append(np.random.rand(3))\n",
    "        return color_map\n",
    "\n",
    "    def show(\n",
    "        self,\n",
    "        point_cloud: np.ndarray,\n",
    "        show_true_label: bool = False, \n",
    "        show_ground: bool = True,\n",
    "        show_clusters: bool = True,\n",
    "        show_unlabeled: bool = True,\n",
    "        show_plane: bool = False,\n",
    "        point_size: \"float | None\" = None\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Show a point cloud with selected label filters.\n",
    "\n",
    "        Args:\n",
    "            point_cloud (np.ndarray): N x 6 array with columns [x, y, z, true_label, pred_label, scanline_index].\n",
    "            show_true_label (bool): If True, use true labels (column 4); otherwise, use predicted labels (column 5).\n",
    "            show_ground (bool): Include ground (label == 9).\n",
    "            show_clusters (bool): Include clusters (label >= 1 and != 9).\n",
    "            show_unlabeled (bool): Include unlabeled (label == 0).\n",
    "            show_plane (bool): Include plane (label == -1).\n",
    "            point_size (float): Override default point size.\n",
    "        \"\"\"\n",
    "\n",
    "        label_col = 3 if show_true_label else 4\n",
    "        labels = point_cloud[:, label_col]\n",
    "\n",
    "        # Construct boolean mask for point selection based on label types\n",
    "        mask = (\n",
    "            (show_plane & (labels == -1)) |\n",
    "            (show_unlabeled & (labels == 0)) |\n",
    "            (show_ground & (labels == 9)) |\n",
    "            (show_clusters & (labels >= 1) & (labels != 9))\n",
    "        )\n",
    "\n",
    "        # Select points and labels based on mask\n",
    "        xyz = point_cloud[mask, :3]\n",
    "        visible_labels = labels[mask].astype(int)\n",
    "\n",
    "        # Map each label to a unique index for pptk visualization\n",
    "        unique_labels = np.unique(visible_labels)\n",
    "        label_to_index = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "        mapped_labels = np.vectorize(label_to_index.get)(visible_labels)\n",
    "\n",
    "        # Generate color map for the unique labels\n",
    "        color_map = self._get_color_map(unique_labels)\n",
    "\n",
    "        # Launch pptk viewer\n",
    "        viewer = pptk.viewer(xyz, mapped_labels)\n",
    "        viewer.set(point_size=point_size or self.point_size)\n",
    "        viewer.color_map(color_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bin file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset('../datasets/semantic-kitti-data')\n",
    "point_cloud, item = dataset[4]\n",
    "\n",
    "point_cloud, normal, d = refine_ground_plane(point_cloud, \n",
    "                                             num_points=NUM_LOWEST_POINTS, \n",
    "                                             height_threshold=SEED_HEIGHT_THRESHOLD, \n",
    "                                             distance_threshold=PLANE_DISTANCE_THRESHOLD, \n",
    "                                             num_iterations=NUM_ITERATIONS)\n",
    "\n",
    "point_cloud = scan_line_run_clustering(point_cloud, SLR_RUN_DISTANCE_THRESHOLD, SLR_MERGE_THRESHOLD)\n",
    "\n",
    "plane = generate_plane_points(point_cloud, normal, d)\n",
    "point_cloud = np.vstack((point_cloud, plane))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizer = PointCloudVisualizer()\n",
    "visualizer.show(point_cloud)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result analisys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'frame_segmentado_1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_50113/2008785573.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m \u001b[0mlista_frames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mframe_segmentado_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe_segmentado_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe_segmentado_3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0mclusters_por_frame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmedia\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesvio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0manalisar_reducao_dimensionalidade\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlista_frames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'frame_segmentado_1' is not defined"
     ]
    }
   ],
   "source": [
    "def calcular_centroides_por_frame(pontos):\n",
    "    \"\"\"\n",
    "    Para um frame, calcula o número de centroides (clusters únicos).\n",
    "    \"\"\"\n",
    "    labels_unicos = np.unique(pontos[:, 4])\n",
    "    return len(labels_unicos)\n",
    "\n",
    "def analisar_reducao_dimensionalidade(lista_frames):\n",
    "    \"\"\"\n",
    "    Para uma lista de frames (cada um com shape N x 6), calcula a redução\n",
    "    da dimensionalidade com base na quantidade de clusters (centroides).\n",
    "\n",
    "    Gera um gráfico com:\n",
    "        - Número de clusters por frame\n",
    "        - Média e desvio padrão\n",
    "    \"\"\"\n",
    "    clusters_por_frame = []\n",
    "\n",
    "    for i, frame in enumerate(lista_frames):\n",
    "        num_clusters = calcular_centroides_por_frame(frame)\n",
    "        clusters_por_frame.append(num_clusters)\n",
    "\n",
    "    clusters_por_frame = np.array(clusters_por_frame)\n",
    "    media = np.mean(clusters_por_frame)\n",
    "    desvio = np.std(clusters_por_frame)\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(clusters_por_frame, marker='o', label='Clusters por frame')\n",
    "    plt.axhline(media, color='green', linestyle='--', label=f'Média = {media:.1f}')\n",
    "    plt.fill_between(\n",
    "        range(len(clusters_por_frame)),\n",
    "        media - desvio,\n",
    "        media + desvio,\n",
    "        color='green',\n",
    "        alpha=0.2,\n",
    "        label=f'Desvio padrão = {desvio:.1f}'\n",
    "    )\n",
    "    plt.title('Redução da Dimensionalidade por Clusterização')\n",
    "    plt.xlabel('Frame')\n",
    "    plt.ylabel('Número de Clusters (Centroides)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return clusters_por_frame, media, desvio\n",
    "\n",
    "\n",
    "lista_frames = list([frame_segmentado_1, frame_segmentado_2, frame_segmentado_3])\n",
    "clusters_por_frame, media, desvio = analisar_reducao_dimensionalidade(lista_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_id_to_name = {\n",
    "    -1: \"plane\",\n",
    "     0: \"unlabeled\",\n",
    "     1: \"car\",\n",
    "     2: \"bicycle\",\n",
    "     3: \"motorcycle\",\n",
    "     4: \"truck\",\n",
    "     5: \"other-vehicle\",\n",
    "     6: \"person\",\n",
    "     7: \"bicyclist\",\n",
    "     8: \"motorcyclist\",\n",
    "     9: \"road\",\n",
    "    10: \"parking\",\n",
    "    11: \"sidewalk\",\n",
    "    12: \"other-ground\",\n",
    "    13: \"building\",\n",
    "    14: \"fence\",\n",
    "    15: \"vegetation\",\n",
    "    16: \"trunk\",\n",
    "    17: \"terrain\",\n",
    "    18: \"pole\",\n",
    "    19: \"traffic-sign\"\n",
    "}\n",
    "\n",
    "def verificar_consistencia_labels(pontos):\n",
    "    \"\"\"\n",
    "    Verifica se todos os pontos de cada cluster (label da posição 4)\n",
    "    têm o mesmo label verdadeiro (posição 3), e retorna os nomes dos\n",
    "    labels verdadeiros quando houver inconsistência.\n",
    "    \n",
    "    Retorna:\n",
    "        inconsistentes (dict): {label_algoritmo: [nomes_dos_labels_verdadeiros]}\n",
    "    \"\"\"\n",
    "    inconsistentes = {}\n",
    "\n",
    "    labels_algoritmo = np.unique(pontos[:, 4])\n",
    "\n",
    "    for label in labels_algoritmo:\n",
    "        cluster = pontos[pontos[:, 4] == label]\n",
    "        labels_verdadeiros = np.unique(cluster[:, 3].astype(int))\n",
    "\n",
    "        if len(labels_verdadeiros) > 1:\n",
    "            nomes_labels = [label_id_to_name.get(l, f\"desconhecido({l})\") for l in labels_verdadeiros]\n",
    "            inconsistentes[int(label)] = nomes_labels\n",
    "\n",
    "    return inconsistentes\n",
    "\n",
    "\n",
    "inconsistencias = verificar_consistencia_labels(frame_segmentado_3)\n",
    "print(inconsistencias.__len__())\n",
    "print('\\n############### Cluster 9 eh o cluster do ground #################\\n')\n",
    "for cluster_id, nomes in inconsistencias.items():\n",
    "    print(f\"Cluster {cluster_id}: {', '.join(nomes)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main('./pointclouds/1504941055.292141000.pcd')\n",
    "main('./pointclouds/1504941060.199916000.pcd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_all_pointclouds(folder_path='./pointclouds'):\n",
    "    \"\"\"\n",
    "    Find and process all .pcd files in the specified folder using the main() function.\n",
    "    \"\"\"\n",
    "    pcd_files = sorted([f for f in os.listdir(folder_path) if f.endswith('.pcd')])\n",
    "\n",
    "    for filename in pcd_files:\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        print(f\"\\nProcessing: {filename}\")\n",
    "        try:\n",
    "            main(file_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {filename}: {e}\")\n",
    "\n",
    "# Run the batch\n",
    "# run_all_pointclouds('./pointclouds')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "segmentation-point-clouds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
