{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ground Plane Fitting and Scan Line Run for 3D LiDAR\n",
    "\n",
    "Ground Plane Fitting (GPF) and Naive Baseline for 3D LiDAR Segmentation\n",
    "\n",
    "This notebook implements ground segmentation using the Ground Plane Fitting (GPF) algorithm \n",
    "proposed in:\n",
    "\n",
    "\"Fast Segmentation of 3D Point Clouds: A Paradigm on LiDAR Data for Autonomous Vehicle Applications\"\n",
    "by D. Zermas, I. Izzat, and N. Papanikolopoulos, 2017.\n",
    "\n",
    "The implementation also includes a naive baseline method for comparison, as well as \n",
    "basic clustering and visualization tools."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pypcd import pypcd  # Use local pypcd version (Python 3 compatible, no lzf support)\n",
    "from sklearn.decomposition import PCA  # Used for plane fitting\n",
    "from sklearn.neighbors import NearestNeighbors  # Used for spatial clustering\n",
    "import pptk  # To install: pip install pptk\n",
    "import os\n",
    "\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import yaml\n",
    "# Note: Ubuntu users may need to fix libz.so.1 symlink issue (see: https://github.com/heremaps/pptk/issues/3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Number of lowest points to use for initial ground seed estimation (LPR)\n",
    "# N_LPR = 1000\n",
    "\n",
    "# # Number of iterations for ground plane refinement\n",
    "# N_ITERATIONS = 6\n",
    "\n",
    "# # Height threshold above the LPR to select initial seed points\n",
    "# TH_SEEDS = 0.4\n",
    "\n",
    "# # Distance threshold to classify a point as ground\n",
    "# TH_DISTANCE_FROM_PLANE = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Baseline Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_ground_extractor(point_cloud, num_lowest_points):\n",
    "    \"\"\"\n",
    "    Naive ground extraction method (baseline).\n",
    "    \n",
    "    This simple method selects the points with the lowest Z values \n",
    "    and assumes they belong to the ground surface. It does not model \n",
    "    the ground plane and is used as a baseline for comparison against \n",
    "    more robust algorithms like Ground Plane Fitting (GPF).\n",
    "    \n",
    "    Args:\n",
    "        point_cloud (np.ndarray): N x 3 or N x D array of point cloud data.\n",
    "        num_lowest_points (int): Number of points with lowest Z values to classify as ground.\n",
    "    \n",
    "    Returns:\n",
    "        ground_indices (np.ndarray): Indices of the selected ground points.\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 1: Sort points by Z axis (height)\n",
    "    sorted_indices = np.argsort(point_cloud[:, 2])\n",
    "\n",
    "    # Step 2: Take the first N indices as ground\n",
    "    ground_indices = sorted_indices[:num_lowest_points]\n",
    "\n",
    "    return ground_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ground Plane Fitting (GPF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpf_extract_initial_seeds_ids(point_cloud, number_of_lowest_points=1000, threshold_seeds=0.4):\n",
    "    \"\"\"\n",
    "    Extract initial seed points for ground plane estimation (GPF).\n",
    "    \n",
    "    Args:\n",
    "        point_cloud (np.ndarray): N x 3 or N x 4 array of points (x, y, z[, class]).\n",
    "        number_of_lowest_points (int): number of lowest Z points to average as LPR.\n",
    "        threshold_seeds (float): threshold to select seeds close to LPR height.\n",
    "    \n",
    "    Returns:\n",
    "        seeds_ids (np.ndarray): indices of points selected as initial seeds.\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 1: Sort the point cloud by Z axis (height)\n",
    "    sorted_indices = np.argsort(point_cloud[:, 2])          # Get indices sorted by height\n",
    "    sorted_points = point_cloud[sorted_indices]             # Apply sorting\n",
    "\n",
    "    # Step 2: Compute LPR (Lowest Point Representative)\n",
    "    LPR_height = np.mean(sorted_points[:number_of_lowest_points, 2])\n",
    "\n",
    "    # Step 3: Select points that are within threshold distance from LPR\n",
    "    seed_mask = sorted_points[:, 2] < (LPR_height + threshold_seeds)\n",
    "    seeds_ids = sorted_indices[seed_mask]\n",
    "\n",
    "    return seeds_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpf_estimate_plane(seed_points):\n",
    "    \"\"\"\n",
    "    Estimate ground plane from seed points using PCA.\n",
    "    \n",
    "    Args:\n",
    "        seed_points (np.ndarray): N x 3 or N x D array of selected ground seed points.\n",
    "    \n",
    "    Returns:\n",
    "        normal_vector (np.ndarray): Unit normal vector [a, b, c] of the estimated plane.\n",
    "        d (float): Plane offset in the equation ax + by + cz + d = 0\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 1: Compute the mean position of the seed points\n",
    "    mean_point = np.mean(seed_points, axis=0)\n",
    "\n",
    "    # Step 2: Center the point cloud around the origin\n",
    "    centered = seed_points - mean_point\n",
    "\n",
    "    # Step 3: Use PCA to estimate the plane's normal vector\n",
    "    pca = PCA(n_components=3)\n",
    "    pca.fit(centered)\n",
    "    normal_vector = pca.components_[-1]  # Last component corresponds to the direction with least variance\n",
    "\n",
    "    # Step 4: Compute the 'd' value using plane equation: n·x + d = 0 → d = -n·x̄\n",
    "    d = -np.dot(normal_vector[:3], mean_point[:3])\n",
    "\n",
    "    return normal_vector[:3], d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpf_refinement(point_cloud, number_of_lowest_points=1000, threshold_seeds=0.4, plane_distance_threshold=0.2, num_iterations=5):\n",
    "    \"\"\"\n",
    "    Iteratively refine the ground plane estimation using seed points and distance threshold.\n",
    "    \n",
    "    Args:\n",
    "        point_cloud (np.ndarray): N x 3 or N x D array of input point cloud.\n",
    "        initial_seed_indices (np.ndarray): Indices of initial ground seed points.\n",
    "        distance_threshold (float): Max allowed point-to-plane distance to be considered ground.\n",
    "        num_iterations (int): Number of iterations for refinement loop.\n",
    "    \n",
    "    Returns:\n",
    "        ground_indices (np.ndarray): Indices of points classified as ground.\n",
    "        non_ground_indices (np.ndarray): Indices of points classified as non-ground.\n",
    "    \"\"\"\n",
    "\n",
    "    xyz_point_cloud = point_cloud[:, :3]\n",
    "    seed_indices = gpf_extract_initial_seeds_ids(xyz_point_cloud, number_of_lowest_points, threshold_seeds)\n",
    "\n",
    "    for _ in range(num_iterations):\n",
    "        # Step 1: Estimate plane from current seed points\n",
    "        normal_vector, d = gpf_estimate_plane(xyz_point_cloud [seed_indices])\n",
    "\n",
    "        # Step 2: Compute point-to-plane distances\n",
    "        numerator = np.abs(np.dot(xyz_point_cloud [:, :3], normal_vector) + d)\n",
    "        denominator = np.linalg.norm(normal_vector)\n",
    "        distances = numerator / denominator\n",
    "\n",
    "        # Step 3: Classify points as ground or non-ground\n",
    "        is_ground = distances < plane_distance_threshold\n",
    "\n",
    "        # Step 4: Update seed indices with the new ground points\n",
    "        seed_indices = np.where(is_ground)[0]\n",
    "\n",
    "    # Final classification after last iteration\n",
    "    ground_indices = np.where(is_ground)[0]\n",
    "    point_cloud[ground_indices, 4] = 9\n",
    "\n",
    "    return point_cloud, normal_vector, d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scan Line Run (SLR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_connected_components(points, eps=0.5, min_samples=1):\n",
    "    \"\"\"\n",
    "    Cluster points into connected components using radius-based neighborhood and Union-Find.\n",
    "\n",
    "    This method groups spatially connected points into clusters. It is a simplified version \n",
    "    of DBSCAN without density-based core points — it only relies on spatial proximity and \n",
    "    minimum cluster size filtering.\n",
    "\n",
    "    Args:\n",
    "        points (np.ndarray): N x 3 (or N x D) array of point coordinates.\n",
    "        eps (float): Maximum distance between neighbors (radius in meters).\n",
    "        min_samples (int): Minimum number of points to form a valid cluster.\n",
    "\n",
    "    Returns:\n",
    "        final_labels (np.ndarray): Cluster labels for each point (-1 for outliers).\n",
    "    \"\"\"\n",
    "    # Step 1: Build neighborhood graph using radius search\n",
    "    neighbors = NearestNeighbors(radius=eps).fit(points)\n",
    "    adjacency = neighbors.radius_neighbors_graph(points, mode='connectivity').tocoo()\n",
    "\n",
    "    # Step 2: Union-Find (Disjoint Set) initialization\n",
    "    parent = np.arange(len(points))\n",
    "\n",
    "    def find(i):\n",
    "        # Path compression\n",
    "        while parent[i] != i:\n",
    "            parent[i] = parent[parent[i]]\n",
    "            i = parent[i]\n",
    "        return i\n",
    "\n",
    "    def union(i, j):\n",
    "        # Union by parent\n",
    "        root_i, root_j = find(i), find(j)\n",
    "        if root_i != root_j:\n",
    "            parent[root_i] = root_j\n",
    "\n",
    "    # Step 3: Connect all neighbors\n",
    "    for i, j in zip(adjacency.row, adjacency.col):\n",
    "        if i != j:\n",
    "            union(i, j)\n",
    "\n",
    "    # Step 4: Assign cluster labels\n",
    "    labels = np.array([find(i) for i in range(len(points))])\n",
    "\n",
    "    # Step 5: Filter small clusters (outliers)\n",
    "    unique_labels, counts = np.unique(labels, return_counts=True)\n",
    "    valid_labels = unique_labels[counts >= min_samples]\n",
    "    mask = np.isin(labels, valid_labels)\n",
    "\n",
    "    # Step 6: Remap valid clusters to consecutive indices, mark outliers as -1\n",
    "    final_labels = -np.ones_like(labels)\n",
    "    final_labels[mask] = np.searchsorted(valid_labels, labels[mask])\n",
    "    return final_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_non_ground_clusters(point_cloud, non_ground_indices, eps=0.5, min_pts=1):\n",
    "    \"\"\"\n",
    "    Apply spatial clustering to points not classified as ground.\n",
    "\n",
    "    This function filters out ground points (using provided indices),\n",
    "    and applies connected components clustering to the remaining non-ground points.\n",
    "\n",
    "    Args:\n",
    "        point_cloud (np.ndarray): N x 4 array with [x, y, z, class] per point.\n",
    "        ground_indices (np.ndarray): Indices of points classified as ground.\n",
    "\n",
    "    Returns:\n",
    "        cluster_labels (np.ndarray): Cluster label for each non-ground point (-1 for noise).\n",
    "        non_ground_indices (np.ndarray): Indices of the non-ground points in the original array.\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 1: Identify non-ground point indices\n",
    "    # total_points = point_cloud.shape[0]\n",
    "    # all_indices = np.arange(total_points)\n",
    "    # non_ground_indices = np.setdiff1d(all_indices, ground_indices)\n",
    "\n",
    "    # Step 2: Extract non-ground points for clustering\n",
    "    non_ground_points = point_cloud[non_ground_indices, :3]  # Use only XYZ for clustering\n",
    "\n",
    "    # Step 3: Apply spatial clustering to non-ground points\n",
    "    cluster_labels = cluster_connected_components(non_ground_points, eps=eps, min_samples=min_pts)\n",
    "\n",
    "\n",
    "    # Step 4: Prepare cluster label column (-1 for all points initially)\n",
    "    cluster_column = -1 * np.ones((point_cloud.shape[0],), dtype=int)\n",
    "\n",
    "    # Step 5: Assign cluster labels to non-ground indices\n",
    "    cluster_column[non_ground_indices] = cluster_labels\n",
    "\n",
    "    # Step 6: Append the cluster labels as a new column\n",
    "    updated_point_cloud = np.hstack((point_cloud, cluster_column.reshape(-1, 1)))\n",
    "\n",
    "    return cluster_labels, non_ground_indices, updated_point_cloud\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizer for Point Clouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_clusters(\n",
    "    point_cloud,\n",
    "    non_ground_indices=None,\n",
    "    cluster_labels=None,\n",
    "    point_size=0.03,\n",
    "    show_ground=True,\n",
    "    show_clusters=True,\n",
    "    show_unlabeled=True,\n",
    "    show_plane=False,\n",
    "    show_true_label=False  # New parameter to toggle label source\n",
    "):\n",
    "    \"\"\"\n",
    "    Visualize a point cloud with consistent class colors using pptk.\n",
    "\n",
    "    Args:\n",
    "        point_cloud (np.ndarray): N x 7 array with [x, y, z, reflectance, gt_label, pred_label, ...].\n",
    "        non_ground_indices (np.ndarray): Indices of non-ground points.\n",
    "        cluster_labels (np.ndarray): Cluster labels for non-ground points (-1 = noise).\n",
    "        point_size (float): Size of points in viewer.\n",
    "        show_ground (bool): Show ground points (label == 1).\n",
    "        show_clusters (bool): Show clustered non-ground points.\n",
    "        show_unlabeled (bool): Show points with label == 0.\n",
    "        show_true_label (bool): Use ground truth labels (col 5) or predicted (col 6).\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Copy the point cloud to avoid modifying original\n",
    "    pc = np.copy(point_cloud)\n",
    "\n",
    "    # Column index to use for visualization\n",
    "    label_col = 3 if show_true_label else 4\n",
    "\n",
    "    # Assign predicted cluster labels if applicable\n",
    "    if cluster_labels is not None and non_ground_indices is not None and show_clusters:\n",
    "        for cluster_id in np.unique(cluster_labels):\n",
    "            if cluster_id == -1:\n",
    "                continue  # skip noise\n",
    "            cluster_point_ids = non_ground_indices[cluster_labels == cluster_id]\n",
    "            pc[cluster_point_ids, label_col] = cluster_id\n",
    "\n",
    "    # Create mask for points to visualize\n",
    "    show_mask = np.full(pc.shape[0], False)\n",
    "    if show_unlabeled:\n",
    "        show_mask |= (pc[:, label_col] == 0)\n",
    "    if show_ground:\n",
    "        show_mask |= (pc[:, label_col] == 1)\n",
    "    if show_clusters:\n",
    "        show_mask |= ((pc[:, label_col] > 1) & (pc[:, label_col] < 100))\n",
    "    if show_plane:\n",
    "        show_mask |= (pc[:, label_col] == 100)\n",
    "\n",
    "    # Extract visible points and labels\n",
    "    xyz = pc[show_mask, :3]\n",
    "    class_labels = pc[show_mask, label_col].astype(int)\n",
    "\n",
    "    # Fixed color palette for known classes\n",
    "    fixed_colors = {\n",
    "        0: [0, 0, 0],\n",
    "        1: [245, 150, 100],\n",
    "        2: [245, 230, 100],\n",
    "        3: [150, 60, 30],\n",
    "        4: [180, 30, 80],\n",
    "        5: [250, 80, 100],\n",
    "        6: [30, 30, 255],\n",
    "        7: [200, 40, 255],\n",
    "        8: [90, 30, 150],\n",
    "        9: [255, 0, 255],\n",
    "        10: [255, 150, 255],\n",
    "        11: [75, 0, 75],\n",
    "        12: [75, 0, 175],\n",
    "        13: [0, 200, 255],\n",
    "        14: [50, 120, 255],\n",
    "        15: [0, 175, 0],\n",
    "        16: [0, 60, 135],\n",
    "        17: [80, 240, 150],\n",
    "        18: [150, 240, 255],\n",
    "        19: [0, 0, 255],\n",
    "        100: [255, 255, 255],\n",
    "    }\n",
    "    # Convert BGR to RGB and normalize\n",
    "    fixed_colors_rgb_normalized = {\n",
    "        label: [bgr[2]/255.0, bgr[1]/255.0, bgr[0]/255.0]  # [R, G, B]\n",
    "        for label, bgr in fixed_colors.items()\n",
    "    }\n",
    "\n",
    "    # Re-map visible class labels to continuous indices for pptk\n",
    "    unique_labels = np.unique(class_labels)\n",
    "    label_to_index = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "    mapped_labels = np.array([label_to_index[label] for label in class_labels])\n",
    "\n",
    "    # Build color map\n",
    "    color_map = []\n",
    "    for label in unique_labels:\n",
    "        if label in fixed_colors_rgb_normalized:\n",
    "            color_map.append(fixed_colors_rgb_normalized[label])\n",
    "        else:\n",
    "            np.random.seed(label)  # keep color stable per class\n",
    "            color_map.append(np.random.rand(3))  # fallback\n",
    "\n",
    "    # Launch viewer\n",
    "    viewer = pptk.viewer(xyz, mapped_labels)\n",
    "    viewer.set(point_size=point_size)\n",
    "    viewer.color_map(color_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(self, data_path, split='train'):\n",
    "        self.data_path = Path(data_path)\n",
    "        self.yaml_path = self.data_path/'semantic-kitti.yaml'\n",
    "        self.velodynes_path = self.data_path/'data_odometry_velodyne/dataset/sequences'\n",
    "        self.labels_path = self.data_path/'data_odometry_labels/dataset/sequences'\n",
    "\n",
    "        self.is_test = (split == 'test')\n",
    "\n",
    "        # Load YAML metadata\n",
    "        with open(self.yaml_path, 'r') as file:\n",
    "            metadata = yaml.safe_load(file)\n",
    "\n",
    "        self.sequences = metadata['split'][split]\n",
    "        self.learning_map = metadata['learning_map']\n",
    "\n",
    "        max_key = max(self.learning_map.keys())\n",
    "\n",
    "        self.learning_map_np = np.zeros((max_key + 1,), dtype=np.uint32)\n",
    "        for k, v in self.learning_map.items():\n",
    "            self.learning_map_np[k] = v\n",
    "\n",
    "        # List all frames\n",
    "        self.frame_paths = []\n",
    "        for sequence in self.sequences:\n",
    "            sequence = f\"{int(sequence):02d}\"\n",
    "            velo_files = sorted((self.velodynes_path/sequence/'velodyne').glob('*.bin'))\n",
    "            for file in velo_files:\n",
    "                self.frame_paths.append((sequence, file.stem))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.frame_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        seq, frame_id = self.frame_paths[idx]\n",
    "\n",
    "        velodyne_path = self.velodynes_path/seq/'velodyne'/f\"{frame_id}.bin\"\n",
    "        label_path = self.labels_path/seq/'labels'/f\"{frame_id}.label\"\n",
    "\n",
    "        # Load point cloud\n",
    "        with open(velodyne_path, 'rb') as file:\n",
    "            point_cloud = np.fromfile(file, dtype=np.float32).reshape(-1, 4)[:, :3]\n",
    "\n",
    "        # Load labels\n",
    "        label = None\n",
    "        mask = None\n",
    "        if not self.is_test and label_path.exists():\n",
    "            with open(label_path, 'rb') as file:\n",
    "                label = np.fromfile(file, dtype=np.uint32) & 0xFFFF\n",
    "            label = self.learning_map_np[label]\n",
    "            mask = label != 0\n",
    "        else:\n",
    "            label = np.zeros(point_cloud.shape[0], dtype=np.uint32)\n",
    "            mask = np.ones(point_cloud.shape[0], dtype=bool)\n",
    "\n",
    "        item = {\n",
    "            'point_cloud': point_cloud,\n",
    "            'label': label,\n",
    "            'mask': mask.astype(bool)\n",
    "        }\n",
    "\n",
    "        point_cloud_with_label = np.hstack((point_cloud, label.reshape(-1, 1)))\n",
    "\n",
    "        # adicionando a coluna para ser a coluna de labels gerada pelo algoritmo\n",
    "        zeros_column = np.zeros((point_cloud.shape[0], 1), dtype=np.float32)\n",
    "        point_cloud_with_label = np.hstack((point_cloud_with_label, zeros_column))\n",
    "\n",
    "        return point_cloud_with_label, item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(pcd_file):\n",
    "    \"\"\"\n",
    "    Process a single point cloud file using GPF + optional clustering.\n",
    "    Visualizes result using pptk.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load PCD file\n",
    "    point_cloud_from_path = pypcd.PointCloud.from_path(pcd_file)\n",
    "\n",
    "    # Create Nx4 array: x, y, z, class\n",
    "    point_cloud = np.stack((\n",
    "        point_cloud_from_path.pc_data['x'],\n",
    "        point_cloud_from_path.pc_data['y'],\n",
    "        point_cloud_from_path.pc_data['z'],\n",
    "        np.zeros((point_cloud_from_path.pc_data.shape[0]))  # default class 0\n",
    "    ), axis=1)\n",
    "\n",
    "    # Step 1: Extract ground seeds using GPF\n",
    "\n",
    "    # Step 2: Refine ground segmentation\n",
    "    point_cloud, normal, d = gpf_refinement(point_cloud, \n",
    "                                                         number_of_lowest_points=N_LPR, \n",
    "                                                         threshold_seeds=TH_SEEDS, \n",
    "                                                         plane_distance_threshold=TH_DISTANCE_FROM_PLANE,\n",
    "                                                         num_iterations=N_ITERATIONS)\n",
    "\n",
    "    # Mark ground points with class = 1\n",
    "    # point_cloud[ground_ids, 3] = 1\n",
    "\n",
    "    # Step 3: Cluster non-ground points (SLR-style)\n",
    "    cluster_labels, non_ground_ids, pc = process_non_ground_clusters(point_cloud, non_ground_ids, eps=0.5, min_pts=1)\n",
    "\n",
    "    # Step 4: Visualize result\n",
    "    visualize_clusters(pc)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize_clusters(point_cloud, non_ground_ids, cluster_labels)\n",
    "# cluster_labels.shape, cluster_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of lowest points to use for initial ground seed estimation (LPR)\n",
    "N_LPR = 2000\n",
    "\n",
    "# Number of iterations for ground plane refinement\n",
    "N_ITERATIONS = 5\n",
    "\n",
    "# Height threshold above the LPR to select initial seed points\n",
    "TH_SEEDS = 0.4\n",
    "\n",
    "# Distance threshold to classify a point as ground\n",
    "TH_DISTANCE_FROM_PLANE = 0.2 # 0.3 eh mto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_plane_points(normal, d, center, size=10, resolution=0.5):\n",
    "    \"\"\"\n",
    "    Gera pontos em um plano com base na equação ax + by + cz + d = 0\n",
    "\n",
    "    Args:\n",
    "        normal (np.ndarray): vetor normal (a, b, c)\n",
    "        d (float): distância do plano\n",
    "        center (np.ndarray): centro do plano (x, y, z)\n",
    "        size (float): tamanho do plano\n",
    "        resolution (float): espaçamento entre os pontos\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Mx3 array de pontos do plano\n",
    "    \"\"\"\n",
    "    a = normal[0]\n",
    "    b = normal[1]\n",
    "    c = normal[2]\n",
    "    # Gera grid ao redor do centro\n",
    "    x_vals = np.arange(center[0] - size, center[0] + size, resolution)\n",
    "    y_vals = np.arange(center[1] - size, center[1] + size, resolution)\n",
    "    xx, yy = np.meshgrid(x_vals, y_vals)\n",
    "    \n",
    "    # Equação do plano: ax + by + cz + d = 0  ->  z = (-d - ax - by)/c\n",
    "    zz = (-d - a * xx - b * yy) / c\n",
    "\n",
    "    xyz = np.stack((xx, yy, zz), axis=-1).reshape(-1, 3)\n",
    "    l1 = np.full((xyz.shape[0], 1), 100, dtype=np.float32)\n",
    "    l2 = np.full((xyz.shape[0], 1), 100, dtype=np.float32)\n",
    "\n",
    "    plane_5d = np.hstack((xyz, l1, l2))\n",
    "    return plane_5d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset('../datasets/semantic-kitti-data')\n",
    "point_cloud, item = dataset[0]\n",
    "# visualize_clusters(point_cloud, show_true_label=True)\n",
    "\n",
    "point_cloud, normal, d = gpf_refinement(point_cloud, \n",
    "                                        number_of_lowest_points=N_LPR, \n",
    "                                        threshold_seeds=TH_SEEDS, \n",
    "                                        plane_distance_threshold=TH_DISTANCE_FROM_PLANE,\n",
    "                                        num_iterations=N_ITERATIONS)\n",
    "\n",
    "center = point_cloud[:, :3].mean(axis=0)\n",
    "plane = generate_plane_points(normal, d, center)\n",
    "point_cloud = np.vstack((point_cloud, plane))\n",
    "\n",
    "visualize_clusters(point_cloud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcd_file = './pointclouds/1504941060.199916000.pcd'\n",
    "\n",
    "point_cloud_from_path = pypcd.PointCloud.from_path(pcd_file)\n",
    "\n",
    "point_cloud = np.stack((point_cloud_from_path.pc_data['x'], \n",
    "                        point_cloud_from_path.pc_data['y'], \n",
    "                        point_cloud_from_path.pc_data['z'], \n",
    "                        np.zeros((point_cloud_from_path.pc_data.shape[0])),\n",
    "                        np.zeros((point_cloud_from_path.pc_data.shape[0])),\n",
    "                        ), \n",
    "                        axis=1)\n",
    "\n",
    "point_cloud, normal, d = gpf_refinement(point_cloud, \n",
    "                                        number_of_lowest_points=N_LPR, \n",
    "                                        threshold_seeds=TH_SEEDS, \n",
    "                                        plane_distance_threshold=TH_DISTANCE_FROM_PLANE,\n",
    "                                        num_iterations=N_ITERATIONS)\n",
    "\n",
    "center = point_cloud[:, :3].mean(axis=0)\n",
    "plane = generate_plane_points(normal, d, center)\n",
    "point_cloud = np.vstack((point_cloud, plane))\n",
    "\n",
    "# cluster_labels, non_ground_ids, pc = process_non_ground_clusters(point_cloud, non_ground_ids, eps=0.5, min_pts=10)\n",
    "\n",
    "# visualize_clusters(point_cloud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main('./pointclouds/1504941055.292141000.pcd')\n",
    "main('./pointclouds/1504941060.199916000.pcd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_all_pointclouds(folder_path='./pointclouds'):\n",
    "    \"\"\"\n",
    "    Find and process all .pcd files in the specified folder using the main() function.\n",
    "    \"\"\"\n",
    "    pcd_files = sorted([f for f in os.listdir(folder_path) if f.endswith('.pcd')])\n",
    "\n",
    "    for filename in pcd_files:\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        print(f\"\\nProcessing: {filename}\")\n",
    "        try:\n",
    "            main(file_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {filename}: {e}\")\n",
    "\n",
    "# Run the batch\n",
    "# run_all_pointclouds('./pointclouds')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "segmentation-point-clouds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
