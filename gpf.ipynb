{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ground Plane Fitting and Scan Line Run for 3D LiDAR\n",
    "\n",
    "Ground Plane Fitting (GPF) and Naive Baseline for 3D LiDAR Segmentation\n",
    "\n",
    "This notebook implements ground segmentation using the Ground Plane Fitting (GPF) algorithm \n",
    "proposed in:\n",
    "\n",
    "\"Fast Segmentation of 3D Point Clouds: A Paradigm on LiDAR Data for Autonomous Vehicle Applications\"\n",
    "by D. Zermas, I. Izzat, and N. Papanikolopoulos, 2017.\n",
    "\n",
    "The implementation also includes a naive baseline method for comparison, as well as \n",
    "basic clustering and visualization tools."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pypcd import pypcd  # Use local pypcd version (Python 3 compatible, no lzf support)\n",
    "from sklearn.decomposition import PCA  # Used for plane fitting\n",
    "from sklearn.neighbors import NearestNeighbors  # Used for spatial clustering\n",
    "import pptk  # To install: pip install pptk\n",
    "import os\n",
    "# Note: Ubuntu users may need to fix libz.so.1 symlink issue (see: https://github.com/heremaps/pptk/issues/3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of lowest points to use for initial ground seed estimation (LPR)\n",
    "N_LPR = 1000\n",
    "\n",
    "# Number of iterations for ground plane refinement\n",
    "N_ITERATIONS = 3\n",
    "\n",
    "# Height threshold above the LPR to select initial seed points\n",
    "TH_SEEDS = 0.4\n",
    "\n",
    "# Distance threshold to classify a point as ground\n",
    "TH_DISTANCE_FROM_PLANE = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Baseline Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_ground_extractor(point_cloud, num_lowest_points):\n",
    "    \"\"\"\n",
    "    Naive ground extraction method (baseline).\n",
    "    \n",
    "    This simple method selects the points with the lowest Z values \n",
    "    and assumes they belong to the ground surface. It does not model \n",
    "    the ground plane and is used as a baseline for comparison against \n",
    "    more robust algorithms like Ground Plane Fitting (GPF).\n",
    "    \n",
    "    Args:\n",
    "        point_cloud (np.ndarray): N x 3 or N x D array of point cloud data.\n",
    "        num_lowest_points (int): Number of points with lowest Z values to classify as ground.\n",
    "    \n",
    "    Returns:\n",
    "        ground_indices (np.ndarray): Indices of the selected ground points.\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 1: Sort points by Z axis (height)\n",
    "    sorted_indices = np.argsort(point_cloud[:, 2])\n",
    "\n",
    "    # Step 2: Take the first N indices as ground\n",
    "    ground_indices = sorted_indices[:num_lowest_points]\n",
    "\n",
    "    return ground_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ground Plane Fitting (GPF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpf_extract_initial_seeds_ids(point_cloud, number_of_lowest_points, thresh_seeds=0.4):\n",
    "    \"\"\"\n",
    "    Extract initial seed points for ground plane estimation (GPF).\n",
    "    \n",
    "    Args:\n",
    "        point_cloud (np.ndarray): N x 3 or N x 4 array of points (x, y, z[, class]).\n",
    "        number_of_lowest_points (int): number of lowest Z points to average as LPR.\n",
    "        thresh_seeds (float): threshold to select seeds close to LPR height.\n",
    "    \n",
    "    Returns:\n",
    "        seeds_ids (np.ndarray): indices of points selected as initial seeds.\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 1: Sort the point cloud by Z axis (height)\n",
    "    sorted_indices = np.argsort(point_cloud[:, 2])          # Get indices sorted by height\n",
    "    sorted_points = point_cloud[sorted_indices]             # Apply sorting\n",
    "\n",
    "    # Step 2: Compute LPR (Lowest Point Representative)\n",
    "    LPR_height = np.mean(sorted_points[:number_of_lowest_points, 2])\n",
    "\n",
    "    # Step 3: Select points that are within threshold distance from LPR\n",
    "    seed_mask = sorted_points[:, 2] < (LPR_height + thresh_seeds)\n",
    "    seeds_ids = sorted_indices[seed_mask]\n",
    "\n",
    "    return seeds_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpf_estimate_plane(seed_points):\n",
    "    \"\"\"\n",
    "    Estimate ground plane from seed points using PCA.\n",
    "    \n",
    "    Args:\n",
    "        seed_points (np.ndarray): N x 3 or N x D array of selected ground seed points.\n",
    "    \n",
    "    Returns:\n",
    "        normal_vector (np.ndarray): Unit normal vector [a, b, c] of the estimated plane.\n",
    "        d (float): Plane offset in the equation ax + by + cz + d = 0\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 1: Compute the mean position of the seed points\n",
    "    mean_point = np.mean(seed_points, axis=0)\n",
    "\n",
    "    # Step 2: Center the point cloud around the origin\n",
    "    centered = seed_points - mean_point\n",
    "\n",
    "    # Step 3: Use PCA to estimate the plane's normal vector\n",
    "    pca = PCA(n_components=3)\n",
    "    pca.fit(centered)\n",
    "    normal_vector = pca.components_[-1]  # Last component corresponds to the direction with least variance\n",
    "\n",
    "    # Step 4: Compute the 'd' value using plane equation: n·x + d = 0 → d = -n·x̄\n",
    "    d = -np.dot(normal_vector[:3], mean_point[:3])\n",
    "\n",
    "    return normal_vector[:3], d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpf_refinement(point_cloud, initial_seed_indices, distance_threshold=0.2, num_iterations=5):\n",
    "    \"\"\"\n",
    "    Iteratively refine the ground plane estimation using seed points and distance threshold.\n",
    "    \n",
    "    Args:\n",
    "        point_cloud (np.ndarray): N x 3 or N x D array of input point cloud.\n",
    "        initial_seed_indices (np.ndarray): Indices of initial ground seed points.\n",
    "        distance_threshold (float): Max allowed point-to-plane distance to be considered ground.\n",
    "        num_iterations (int): Number of iterations for refinement loop.\n",
    "    \n",
    "    Returns:\n",
    "        ground_indices (np.ndarray): Indices of points classified as ground.\n",
    "        non_ground_indices (np.ndarray): Indices of points classified as non-ground.\n",
    "    \"\"\"\n",
    "\n",
    "    seed_indices = initial_seed_indices\n",
    "\n",
    "    for _ in range(num_iterations):\n",
    "        # Step 1: Estimate plane from current seed points\n",
    "        normal_vector, d = gpf_estimate_plane(point_cloud[seed_indices])\n",
    "\n",
    "        # Step 2: Compute point-to-plane distances\n",
    "        numerator = np.abs(np.dot(point_cloud[:, :3], normal_vector) + d)\n",
    "        denominator = np.linalg.norm(normal_vector)\n",
    "        distances = numerator / denominator\n",
    "\n",
    "        # Step 3: Classify points as ground or non-ground\n",
    "        is_ground = distances < distance_threshold\n",
    "        is_not_ground = ~is_ground\n",
    "\n",
    "        # Step 4: Update seed indices with the new ground points\n",
    "        seed_indices = np.where(is_ground)[0]\n",
    "\n",
    "    # Final classification after last iteration\n",
    "    ground_indices = np.where(is_ground)[0]\n",
    "    non_ground_indices = np.where(is_not_ground)[0]\n",
    "\n",
    "    return ground_indices, non_ground_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scan Line Run (SLR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_connected_components(points, eps=0.5, min_samples=1):\n",
    "    \"\"\"\n",
    "    Cluster points into connected components using radius-based neighborhood and Union-Find.\n",
    "\n",
    "    This method groups spatially connected points into clusters. It is a simplified version \n",
    "    of DBSCAN without density-based core points — it only relies on spatial proximity and \n",
    "    minimum cluster size filtering.\n",
    "\n",
    "    Args:\n",
    "        points (np.ndarray): N x 3 (or N x D) array of point coordinates.\n",
    "        eps (float): Maximum distance between neighbors (radius in meters).\n",
    "        min_samples (int): Minimum number of points to form a valid cluster.\n",
    "\n",
    "    Returns:\n",
    "        final_labels (np.ndarray): Cluster labels for each point (-1 for outliers).\n",
    "    \"\"\"\n",
    "    # Step 1: Build neighborhood graph using radius search\n",
    "    neighbors = NearestNeighbors(radius=eps).fit(points)\n",
    "    adjacency = neighbors.radius_neighbors_graph(points, mode='connectivity').tocoo()\n",
    "\n",
    "    # Step 2: Union-Find (Disjoint Set) initialization\n",
    "    parent = np.arange(len(points))\n",
    "\n",
    "    def find(i):\n",
    "        # Path compression\n",
    "        while parent[i] != i:\n",
    "            parent[i] = parent[parent[i]]\n",
    "            i = parent[i]\n",
    "        return i\n",
    "\n",
    "    def union(i, j):\n",
    "        # Union by parent\n",
    "        root_i, root_j = find(i), find(j)\n",
    "        if root_i != root_j:\n",
    "            parent[root_i] = root_j\n",
    "\n",
    "    # Step 3: Connect all neighbors\n",
    "    for i, j in zip(adjacency.row, adjacency.col):\n",
    "        if i != j:\n",
    "            union(i, j)\n",
    "\n",
    "    # Step 4: Assign cluster labels\n",
    "    labels = np.array([find(i) for i in range(len(points))])\n",
    "\n",
    "    # Step 5: Filter small clusters (outliers)\n",
    "    unique_labels, counts = np.unique(labels, return_counts=True)\n",
    "    valid_labels = unique_labels[counts >= min_samples]\n",
    "    mask = np.isin(labels, valid_labels)\n",
    "\n",
    "    # Step 6: Remap valid clusters to consecutive indices, mark outliers as -1\n",
    "    final_labels = -np.ones_like(labels)\n",
    "    final_labels[mask] = np.searchsorted(valid_labels, labels[mask])\n",
    "    return final_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_non_ground_clusters(point_cloud, non_ground_indices, eps=0.5, min_pts=1):\n",
    "    \"\"\"\n",
    "    Apply spatial clustering to points not classified as ground.\n",
    "\n",
    "    This function filters out ground points (using provided indices),\n",
    "    and applies connected components clustering to the remaining non-ground points.\n",
    "\n",
    "    Args:\n",
    "        point_cloud (np.ndarray): N x 4 array with [x, y, z, class] per point.\n",
    "        ground_indices (np.ndarray): Indices of points classified as ground.\n",
    "\n",
    "    Returns:\n",
    "        cluster_labels (np.ndarray): Cluster label for each non-ground point (-1 for noise).\n",
    "        non_ground_indices (np.ndarray): Indices of the non-ground points in the original array.\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 1: Identify non-ground point indices\n",
    "    # total_points = point_cloud.shape[0]\n",
    "    # all_indices = np.arange(total_points)\n",
    "    # non_ground_indices = np.setdiff1d(all_indices, ground_indices)\n",
    "\n",
    "    # Step 2: Extract non-ground points for clustering\n",
    "    non_ground_points = point_cloud[non_ground_indices, :3]  # Use only XYZ for clustering\n",
    "\n",
    "    # Step 3: Apply spatial clustering to non-ground points\n",
    "    cluster_labels = cluster_connected_components(non_ground_points, eps=eps, min_samples=min_pts)\n",
    "\n",
    "    return cluster_labels, non_ground_indices\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizer for Point Clouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_clusters(\n",
    "    point_cloud,\n",
    "    non_ground_indices=None,\n",
    "    cluster_labels=None,\n",
    "    point_size=0.03,\n",
    "    show_ground=True,\n",
    "    show_clusters=True,\n",
    "    show_unlabeled=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Visualize a point cloud with consistent class colors using pptk.\n",
    "\n",
    "    Args:\n",
    "        point_cloud (np.ndarray): N x 4 array with [x, y, z, class] per point.\n",
    "        non_ground_indices (np.ndarray): Indices of non-ground points.\n",
    "        cluster_labels (np.ndarray): Cluster labels for non-ground points (-1 = noise).\n",
    "        point_size (float): Size of points in viewer.\n",
    "        cluster_offset (int): Class offset where clusters begin (default: 2).\n",
    "        show_ground (bool): Show ground points (class == 1).\n",
    "        show_clusters (bool): Show clustered non-ground points.\n",
    "        show_unlabeled (bool): Show points with class == 0.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Copy the point cloud to avoid modifying original\n",
    "    pc = np.copy(point_cloud)\n",
    "\n",
    "    # Assign cluster labels if provided\n",
    "    if cluster_labels is not None and non_ground_indices is not None and show_clusters:\n",
    "        for cluster_id in np.unique(cluster_labels):\n",
    "            if cluster_id == -1:\n",
    "                continue  # skip noise\n",
    "            cluster_point_ids = non_ground_indices[cluster_labels == cluster_id]\n",
    "            pc[cluster_point_ids, 3] = cluster_id\n",
    "\n",
    "    # Create mask for points to visualize\n",
    "    show_mask = np.full(pc.shape[0], False)\n",
    "\n",
    "    if show_unlabeled:\n",
    "        show_mask |= (pc[:, 3] == 0)\n",
    "\n",
    "    if show_ground:\n",
    "        show_mask |= (pc[:, 3] == 1)\n",
    "\n",
    "    if show_clusters and cluster_labels is not None:\n",
    "        show_mask |= (pc[:, 3] > 1)\n",
    "\n",
    "    # Extract visible points and labels\n",
    "    xyz = pc[show_mask, :3]\n",
    "    class_labels = pc[show_mask, 3].astype(int)\n",
    "\n",
    "    # Fixed color palette per class\n",
    "    fixed_colors = {\n",
    "        0: [1.0, 1.0, 1.0],  # unlabeled - white\n",
    "        1: [1.0, 0.0, 1.0],  # ground    - violet\n",
    "    }\n",
    "\n",
    "    # Re-map visible class labels to continuous local indices for pptk\n",
    "    unique_labels = np.unique(class_labels)\n",
    "    label_to_index = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "    mapped_labels = np.array([label_to_index[label] for label in class_labels])\n",
    "\n",
    "    # Build color map in the local label space\n",
    "    color_map = []\n",
    "    for label in unique_labels:\n",
    "        if label in fixed_colors:\n",
    "            color_map.append(fixed_colors[label])\n",
    "        else:\n",
    "            np.random.seed(label)  # keep color stable per class\n",
    "            color_map.append(np.random.rand(3))  # fallback\n",
    "\n",
    "    # Launch viewer\n",
    "    viewer = pptk.viewer(xyz, mapped_labels)\n",
    "    viewer.set(point_size=point_size)\n",
    "    viewer.color_map(color_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(pcd_file):\n",
    "    \"\"\"\n",
    "    Process a single point cloud file using GPF + optional clustering.\n",
    "    Visualizes result using pptk.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load PCD file\n",
    "    point_cloud_from_path = pypcd.PointCloud.from_path(pcd_file)\n",
    "\n",
    "    # Create Nx4 array: x, y, z, class\n",
    "    point_cloud = np.stack((\n",
    "        point_cloud_from_path.pc_data['x'],\n",
    "        point_cloud_from_path.pc_data['y'],\n",
    "        point_cloud_from_path.pc_data['z'],\n",
    "        np.zeros((point_cloud_from_path.pc_data.shape[0]))  # default class 0\n",
    "    ), axis=1)\n",
    "\n",
    "    # Step 1: Extract ground seeds using GPF\n",
    "    seeds_ids = gpf_extract_initial_seeds_ids(point_cloud, N_LPR, TH_SEEDS)\n",
    "\n",
    "    # Step 2: Refine ground segmentation\n",
    "    ground_ids, non_ground_ids = gpf_refinement(point_cloud, seeds_ids, TH_DISTANCE_FROM_PLANE, N_ITERATIONS)\n",
    "\n",
    "    # Mark ground points with class = 1\n",
    "    point_cloud[ground_ids, 3] = 1\n",
    "\n",
    "    # Step 3: Cluster non-ground points (SLR-style)\n",
    "    cluster_labels, non_ground_ids = process_non_ground_clusters(point_cloud, non_ground_ids, eps=0.5, min_pts=1)\n",
    "\n",
    "    # Step 4: Visualize result\n",
    "    visualize_clusters(point_cloud, non_ground_ids, cluster_labels)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcd_file = './pointclouds/1504941060.199916000.pcd'\n",
    "\n",
    "point_cloud_from_path = pypcd.PointCloud.from_path(pcd_file)\n",
    "\n",
    "point_cloud = np.stack((point_cloud_from_path.pc_data['x'], \n",
    "                        point_cloud_from_path.pc_data['y'], \n",
    "                        point_cloud_from_path.pc_data['z'], \n",
    "                        np.zeros((point_cloud_from_path.pc_data.shape[0]))), \n",
    "                        axis=1)\n",
    "\n",
    "\n",
    "seeds_ids = gpf_extract_initial_seeds_ids(point_cloud, N_LPR, TH_SEEDS)\n",
    "ground_ids, non_ground_ids = gpf_refinement(point_cloud, seeds_ids, TH_DISTANCE_FROM_PLANE, N_ITERATIONS)\n",
    "point_cloud[ground_ids, 3] = 1 # 1 for ground points\n",
    "cluster_labels, non_ground_ids = process_non_ground_clusters(point_cloud, non_ground_ids, eps=0.5, min_pts=1)\n",
    "\n",
    "visualize_clusters(point_cloud, non_ground_ids, cluster_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "main('./pointclouds/1504941055.292141000.pcd')\n",
    "main('./pointclouds/1504941060.199916000.pcd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_all_pointclouds(folder_path='./pointclouds'):\n",
    "    \"\"\"\n",
    "    Find and process all .pcd files in the specified folder using the main() function.\n",
    "    \"\"\"\n",
    "    pcd_files = sorted([f for f in os.listdir(folder_path) if f.endswith('.pcd')])\n",
    "\n",
    "    for filename in pcd_files:\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        print(f\"\\nProcessing: {filename}\")\n",
    "        try:\n",
    "            main(file_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {filename}: {e}\")\n",
    "\n",
    "# Run the batch\n",
    "# run_all_pointclouds('./pointclouds')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "segmentation-point-clouds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
